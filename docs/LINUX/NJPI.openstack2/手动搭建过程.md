

# openstack_stein_2node_install 1 - 基础环境搭建

## 1、系统拓扑图

1. 系统拓扑图
   ![](./_v_images/20191014011328239_541166351.png)
2. 说明：

 * 本次配置为两个节点的openstack
 * controller与compute节点都有两块网卡ens33,ens37
 * ens37连接192.168.10.0/24网络，此网络为管理网络（Management Network）
 * ens33连接10.10.10.0/24网络，此网络为provider网络（provider Network）,改网络为实例提供外网。改网络接口不需要配置地址。

    3. **注意：**

  * ens33如果开始时不配置网络地址，则ens37必须能够上网，否则不能下载安装包
  * 如果ens37不能够上网，只有ens33能够上网，则在配置开始时，对ens33设置ip地址，让controller节点能够连接internet，在openstack配置完成后，删除ens33的ip地址。本例中采用此方案。如下图所示：

  >![](./_v_images/20191014011433989_789616212.png)

## 2、基本环节搭建

### 2.1 安全(security)

1. openstack需要各种服务，每个服务都需要设置密码。
2. 设置密码，可以使用以下代码生成随机密码：

```shell
root@controlller ~]# openssl rand -hex 10
ea4f99cd40067b6dcfad
```

3. openstack需要设置密码的服务

| Password name                        | Description                                      | 密码                 |
| ------------------------------------ | ------------------------------------------------ | -------------------- |
| Database password (no variable used) | Root password for the database                   | ea4f99cd40067b6dcfad |
| ADMIN_PASS                           | Password of user admin                           | ea4f99cd40067b6dcfad |
| CINDER_DBPASS                        | Database password for the Block Storage service  | ea4f99cd40067b6dcfad |
| CINDER_PASS                          | Password of Block Storage service user cinder    | ea4f99cd40067b6dcfad |
| DASH_DBPASS                          | Database password for the Dashboard              | ea4f99cd40067b6dcfad |
| DEMO_PASS                            | Password of user demo                            | ea4f99cd40067b6dcfad |
| GLANCE_DBPASS                        | Database password for Image service              | ea4f99cd40067b6dcfad |
| GLANCE_PASS                          | Password of Image service user glance            | ea4f99cd40067b6dcfad |
| KEYSTONE_DBPASS                      | Database password of Identity service            | ea4f99cd40067b6dcfad |
| METADATA_SECRET                      | Secret for the metadata proxy                    | ea4f99cd40067b6dcfad |
| NEUTRON_DBPASS                       | Database password for the Networking service     | ea4f99cd40067b6dcfad |
| NEUTRON_PASS                         | Password of Networking service user neutron      | ea4f99cd40067b6dcfad |
| NOVA_DBPASS                          | Database password for Compute service            | ea4f99cd40067b6dcfad |
| NOVA_PASS                            | Password of Compute service user nova            | ea4f99cd40067b6dcfad |
| PLACEMENT_PASS                       | Password of the Placement service user placement | ea4f99cd40067b6dcfad |
| RABBIT_PASS                          | Password of RabbitMQ user openstack              | ea4f99cd40067b6dcfad |

4. **说明：**
   可以对以上密码设置一个统一的密码，如：openstack

### 2.2主机网络（host networking)

#### 2.2.1controller节点配置

1. 管理网络接口配置(ens37)配置：1. 管理网络接口配置(ens37)配置：g

```shell
root@controlller ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens37
TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yegs
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
UUID=af61a7aa-1a75-3758-8b1d-2e6acf6ace95
NAME=ens37
DEVICE=ens37
ONBOOT=yes
IPADDR=192.168.10.17
NETMASK=255.255.255.0
```

2. provider 网络接口(ens33）配置

```shell
root@controlller ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens33
TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=ens33
DEVICE=ens33
ONBOOT=yes
IPADDR=10.10.10.17
NETMASK=255.255.255.0
GATEWAY=10.10.10.2
DNS1=8.8.8.8
```

3. /etc/hosts配置

```shell
root@controlller ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.10.17 controller
192.168.10.18 compute
```

**说明：**
设置管理网络地址，不要设置provider网络地址

#### 2.2.2 Compute节点配置

1. 管理网络接口(ens37）配置

```shell
[root@compute ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens37
TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
UUID=af61a7aa-1a75-3758-8b1d-2e6acf6ace95
NAME=ens37
DEVICE=ens37
ONBOOT=yes
IPADDR=192.168.10.18
NETMASK=255.255.255.02
```

 2 . provider 网络接口(ens33）配置：

 ```shell
 [root@compute ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens33
TYPE=Ethernet
BOOTPROTO=static
DEFROUTE=yes
PEERDNS=yes
PEERROUTES=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_PEERDNS=yes
IPV6_PEERROUTES=yes
IPV6_FAILURE_FATAL=no
NAME=ens33
DEVICE=ens33
ONBOOT=yes
IPADDR=10.10.10.18
NETMASK=255.255.255.0
GATEWAY=10.10.10.2
DNS1=8.8.8.8
 ```

    3. /etc/hosts配置
       拷贝 controller节点 /etc/hosts文件到compute节点，命令如下： 

 ```shell
[root@compute ~]# scp root@192.168.10.17:/etc/hosts /etc/hosts
The authenticity of host '192.168.10.17 (192.168.10.17)' can't be established.
ECDSA key fingerprint is SHA256:YHPabnLNSR7Bljly0ZgI2PxXAasnYD9K0lsQRM38kTk.
ECDSA key fingerprint is MD5:8d:be:fd:de:3a:1d:20:dc:e1:ae:b4:17:c3:a5:2f:e4.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.10.17' (ECDSA) to the list of known hosts.
root@192.168.10.17's password:
hosts                                         100%  205   182.9KB/s   00:00
 ```

#### 2.2.3测试连通型

1. controller 能否ping通 compute节点与外网

```shell
[root@controlller ~]# ping compute
PING compute (192.168.10.18) 56(84) bytes of data.
64 bytes from compute (192.168.10.18): icmp_seq=1 ttl=64 time=0.639 ms
64 bytes from compute (192.168.10.18): icmp_seq=2 ttl=64 time=0.483 ms
64 bytes from compute (192.168.10.18): icmp_seq=3 ttl=64 time=0.682 ms
64 bytes from compute (192.168.10.18): icmp_seq=4 ttl=64 time=0.833 ms
64 bytes from compute (192.168.10.18): icmp_seq=5 ttl=64 time=0.744 ms
--- compute ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4004ms
rtt min/avg/max/mdev = 0.483/0.676/0.833/0.117 ms
[root@controlller ~]# ping 10.10.10.18
PING 10.10.10.18 (10.10.10.18) 56(84) bytes of data.
64 bytes from 10.10.10.18: icmp_seq=1 ttl=64 time=0.771 ms
64 bytes from 10.10.10.18: icmp_seq=2 ttl=64 time=0.550 ms
64 bytes from 10.10.10.18: icmp_seq=3 ttl=64 time=0.678 ms
^C
--- 10.10.10.18 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2005ms
rtt min/avg/max/mdev = 0.550/0.666/0.771/0.093 ms
[root@controlller ~]# ping www.baidu.com
PING www.a.shifen.com (183.232.231.174) 56(84) bytes of data.
64 bytes from 183.232.231.174 (183.232.231.174): icmp_seq=1 ttl=128 time=42.8 ms
64 bytes from 183.232.231.174 (183.232.231.174): icmp_seq=2 ttl=128 time=42.6 ms
^C
--- www.a.shifen.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1005ms
rtt min/avg/max/mdev = 42.662/42.762/42.862/0.100 ms
```

2. compute节点能否 ping通 controller节点与外网

```shell
[root@compute ~]# ping controller
PING controller (192.168.10.17) 56(84) bytes of data.
64 bytes from controller (192.168.10.17): icmp_seq=1 ttl=64 time=0.497 ms
64 bytes from controller (192.168.10.17): icmp_seq=2 ttl=64 time=0.734 ms
^C
--- controller ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.497/0.615/0.734/0.121 ms
[root@compute ~]# ping 10.10.10.17
PING 10.10.10.17 (10.10.10.17) 56(84) bytes of data.
64 bytes from 10.10.10.17: icmp_seq=1 ttl=64 time=0.478 ms
64 bytes from 10.10.10.17: icmp_seq=2 ttl=64 time=0.598 ms
^C
--- 10.10.10.17 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.478/0.538/0.598/0.060 ms
[root@compute ~]# ping www.baidu.com
PING www.a.shifen.com (183.232.231.172) 56(84) bytes of data.
64 bytes from 183.232.231.172 (183.232.231.172): icmp_seq=1 ttl=128 time=29.2 ms
64 bytes from 183.232.231.172 (183.232.231.172): icmp_seq=2 ttl=128 time=30.2 ms
^C
--- www.a.shifen.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1003ms
rtt min/avg/max/mdev = 29.231/29.720/30.210/0.518 ms
```

**说明：**
所有测试必须能够ping通

### 2.3 配置NTP服务

#### 2.3.1controller节点配置

1. 安装chrony包

```shell
# yum install chrony
```

2. 配置/etc/chrony.conf文件
   增加两行内容：

 ```shell
 server 192.168.10.17 iburst
 allow 192.168.10.0/24
 ```

    3. 设置服务自启动与启动服务

 ```shell
 # systemctl enable chronyd.service
# systemctl start chronyd.service
 ```

 ####  2.3.2 compute节点配置

1. 安装chrony包

```shell
# yum install chrony
```

2. 配置/etc/chrony.conf文件
   **删除所有的server，增加1行**

 ```shell
 server controller  iburst
 ```

    3. 设置服务自启动与启动服务

 ```shell
 # systemctl enable chronyd.service
# systemctl start chronyd.service
 ```

#### 2.3.3 测试

1. controller节点

```shell
[root@controlller ~]# chronyc sources
210 Number of sources = 5
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^- controller                    3   6   377    33    -16us[  -16us] +/-   21ms
^* 203.107.6.88                  2   8   377    90   -176us[ -227us] +/-   21ms
^? ntp1.ams1.nl.leaseweb.net     2   9     0   39m    +14ms[ -9633s] +/-  202ms
^- ntp.xtom.nl                   2   7   377    18  +6113us[+6113us] +/-  111ms
^- electrode.felixc.at           3   7   307    89    +12ms[  +12ms] +/-  182
```

2. compute节点

```shell
root@compute yum.repos.d]# chronyc sources
210 Number of sources = 1
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^? controller                    0   7     0     -     +0ns[   +0ns] +/-    0ns
```

### 2.4  openstack packages配置

**对于所有节点（controller,comute节点）安装**

#### 2.4.1 安装openstack respositor

1. stein版本

```shell
 [root@controlller ~]# yum install centos-release-openstack-stein
```

2. rocky版本

```shell
 [root@controlller ~]# yum install centos-release-openstack-rocky
```

3. queen版本

```shell
 [root@controlller ~]# yum install centos-release-openstack-queen
```

安装完成后，在/etc/yum.repos.d文件夹下新增以下的repo文件：

```shell
 root@controlller yum.repos.d]# ls
CentOS-NFS-Ganesha-28.repo  CentOS-OpenStack-stein.repo
CentOS-Ceph-Nautilus.repo  CentOS-QEMU-EV.repo
CentOS-Storage-common.repo
```

#### 2.4.2 安装其他包

1. 更新系统

```shell
 [root@controlller yum.repos.d]# yum upgrade
```

2. 安装openstack client

```shell
[root@controlller yum.repos.d]# yum install python-openstackclient
```

3. 安装 openstack-selinux
   对于centos来说，sexlinux，默认处于打开状态，有两种解决办法:

>  (1) 禁止sexlinux，打开/etc/selinux/conf,设置：
>
>  ```shell
>  SELINUX=disabled
>  ```
>
>  (2)安装openstack-selinux，解决selinux限制
>
>  ```shell
>  # yum install openstack-selinux
>  ```

### 2.5 安装数据库（mariadb）

**在controller节点安装数据库**

#### 2.5.1 安装mariadb

```shell
# yum install mariadb mariadb-server python2-PyMySQL
```

#### 2.5.2 配置mysql

1. 在/etc/my.cnf.d下创建文件openstack.cnf,内容如下：

```shell
bind-address = 192.168.10.17

default-storage-engine = innodb
innodb_file_per_table = on
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8
```

**注意：如果启动客户端报错，请在bind-addres行前面增加一行，内容为：[mysqld]**

2. 配置自启动与启动mysql服务

```shell
# systemctl enable mariadb.service
# systemctl start mariadb.service
```

3. 安装数据库,设置root用户密码为：123456

```shell
[root@controlller my.cnf.d]# mysql_secure_installation

NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB
      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!

In order to log into MariaDB to secure it, we'll need the current
password for the root user.  If you've just installed MariaDB, and
you haven't set the root password yet, the password will be blank,
so you should just press enter here.

Enter current password for root (enter for none):
OK, successfully used password, moving on...

Setting the root password ensures that nobody can log into the MariaDB
root user without the proper authorisation.

Set root password? [Y/n] Y
New password:
Re-enter new password:
Password updated successfully!
Reloading privilege tables..
 ... Success!


By default, a MariaDB installation has an anonymous user, allowing anyone
to log into MariaDB without having to have a user account created for
them.  This is intended only for testing, and to make the installation
go a bit smoother.  You should remove them before moving into a
production environment.

Remove anonymous users? [Y/n] Y
 ... Success!

Normally, root should only be allowed to connect from 'localhost'.  This
ensures that someone cannot guess at the root password from the network.

Disallow root login remotely? [Y/n] n
 ... skipping.

By default, MariaDB comes with a database named 'test' that anyone can
access.  This is also intended only for testing, and should be removed
before moving into a production environment.

Remove test database and access to it? [Y/n] Y
 - Dropping test database...
 ... Success!
 - Removing privileges on test database...
 ... Success!

Reloading the privilege tables will ensure that all changes made so far
will take effect immediately.

Reload privilege tables now? [Y/n] Y
 ... Success!

Cleaning up...

All done!  If you've completed all of the above steps, your MariaDB
installation should now be secure.

Thanks for using MariaDB!
```

4. 设置客户端免密登录
   正常情况下使用mysql客户端命令登录mysql需要输入用户名与密码，可以在/etc/my.cnf.d/client.conf下配置：

```shell
[client]

# This group is not read by mysql client library,
# If you use the same .cnf file for MySQL and MariaDB,
# use it for MariaDB-only client options
user=root
password="123456"
```

这样只有在终端中输入mysql就可以直接登录了

```shell
root@controlller my.cnf.d]# mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 22
Server version: 10.3.10-MariaDB MariaDB Server

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]>
```

### 2.6安装消息队列

openstack中使用消息队列，消息队列用于controller节点与compute之间的消息通信，openstack使用消息队列来协调各种服务之间的操作与状态信息，消息队列服务通常安装在contoller节点上。消息队列软件有：RabbitMQ，Qpid，ZeroMQ,本次使用RabbitMQ
**在controller节点上安装**

#### 2.6.1安装rabbitmq

```shell
[root@controlller ~]# yum install rabbitmq-server -y
```

#### 2.6.2设置开机自启动与启动服务

```shell
root@controlller ~]#  systemctl enable rabbitmq-server.service
Created symlink from /etc/systemd/system/multi-user.target.wants/rabbitmq-server.service to /usr/lib/systemd/system/rabbitmq-server.service.
[root@controlller ~]# systemctl start rabbitmq-server.service
[root@controlller ~]#
```

#### 2.6.3 设置消息队列用户名与密码

1. 设置用户与密码

```shell
[root@controlller ~]# rabbitmqctl  add_user openstack ea4f99cd40067b6dcfad
Creating user "openstack"
```

2. 查看设置的用户与密码

```shell
root@controlller ~]# rabbitmqctl list_users
Listing users
openstack	[]
guest	[administrator]
```

#### 2.6.4设置用户权限

1. 语法：

```shell
rabbitmqctl set_permissions <user> <conf> <write> <read>
```

2. 说明：

>  * <user>是指为某个用户设置权限
>  * <conf>设置配置权限
>  * <write>设置写权限
>  * <read>设置度权限

3. 对openstack用户设置权限

```shell
[root@controlller ~]#rabbitmqctl set_permissions openstack ".*" ".*" ".*"
Setting permissions for user "openstack" in vhost "/"
```

**说明：**
**对openstack用户设置了所有的配置，写与读的权限**

4. 查看openstack用户的权限

```shell
[root@controlller ~]# rabbitmqctl list_user_permissions openstack
Listing permissions for user "openstack"
/	.*	.*	.*
```

### 2.7 安装缓存服务(MemCached)

openstack为其他服务提供身份识别服务，需要使用Memcached来缓存tokens，缓存服务通常安装在controller节点上。
**在controller节点上安装**

#### 2.7.1 安装memcached包

```shell
[root@controlller ~]# yum install memcached python-memcached -y
```

#### 2.7.2 配置memcached

编辑/etc/sysconfig/memcached文件，修改：

```shell
OPTIONS="-l 127.0.0.1,::1"
```

为：

```shell
OPTIONS="-l 127.0.0.1,::1,controller"
```

#### 2.7.3设置自启动与启动服务

```shell
[root@controlller ~]# systemctl enable memcached.service
Created symlink from /etc/systemd/system/multi-user.target.wants/memcached.service to /usr/lib/systemd/system/memcached.service.
[root@controlller ~]# systemctl start memcached.service
[root@controlller ~]#
```

查看服务是否启动成功：

```shell
[root@controlller ~]# systemctl status memcached.service
```

### 2.8 配置Etcd

Etcd是一种可靠的分布式键值对管理软件，可以存储分布式键锁，存储配置信息，跟踪服务的是否存活等应用场景。openstack服务会用到Etcd软件。
**etcd服务安装在controller节点上**

#### 2.8.1安装包

```shell
[root@controlller ~]# yum install etcd
```

#### 2.8.2 配置etcd.conf

编辑/etc/etcd/etcd.conf,内容如下：

```shell
#[Member]
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.10.17:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.10.17:2379"
ETCD_NAME="controller"
#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.10.17:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.10.17:2379"
ETCD_INITIAL_CLUSTER="controller=http://192.168.10.17:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"
ETCD_INITIAL_CLUSTER_STATE="new"
```

#### 2.8.3 设置自启动与启动服务

```shell
# systemctl enable etcd
# systemctl start etcd
```

查看服务是否启动成功：
x

```
[root@controlller ~]# systemctl status etcd
● etcd.service - Etcd Server
   Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled)
   Active: active (running) since 日 2019-10-13 23:01:17 CST; 1min 0s ago
 Main PID: 43139 (etcd)
   CGroup: /system.slice/etcd.service
           └─43139 /usr/bin/etcd --name=controller --data-dir=/var/lib/etcd/d...

10月 13 23:01:17 controlller etcd[43139]: 850ca1f0fc28aca8 received MsgVote...2
10月 13 23:01:17 controlller etcd[43139]: 850ca1f0fc28aca8 became leader at...2
10月 13 23:01:17 controlller etcd[43139]: raft.node: 850ca1f0fc28aca8 elect...2
10月 13 23:01:17 controlller etcd[43139]: published {Name:controller Client...b
10月 13 23:01:17 controlller etcd[43139]: setting up the initial cluster ve...3
10月 13 23:01:17 controlller systemd[1]: Started Etcd Server.
10月 13 23:01:17 controlller etcd[43139]: set the initial cluster version t...3
10月 13 23:01:17 controlller etcd[43139]: enabled capabilities for version 3.3
10月 13 23:01:17 controlller etcd[43139]: ready to serve client requests
10月 13 23:01:17 controlller etcd[43139]: serving insecure client requests ...!
Hint: Some lines were ellipsized, use -l to show in full.
```

   

# openstack_stein_2node_install-2 

## 3、安装认证服务
**在控制节点上安装**
### 3.1 认证服务概述
#### 3.1.1keystone对象
1. User：指使用Openstack service的用户，可以是人、服务、系统，但凡使用了Openstack service的对象都可以称为User。
2. Project(Tenant)：可以理解为一个人、或服务所拥有的 资源集合 。在一个Project(Tenant)中可以包含多个User，每一个User都会根据权限的划分来使用Project(Tenant)中的资源。比如通过Nova创建虚拟机时要指定到某个Project中，在Cinder创建卷也要指定到某个Project中。User访问Project的资源前，必须要与该Project关联，并且指定User在Project下的Role。
3. Role：用于划分权限。可以通过给User指定Role，使User获得Role对应的操作权限。Keystone返回给User的Token包含了Role列表，被访问的Services会判断访问它的User和User提供的Token中所包含的Role。系统默认使用管理Role admin和成员Role _member_ 。
4. Token：是一个字符串表示，作为访问资源的令牌。Token包含了在 指定范围和有效时间内 可以被访问的资源。
5. Credentials：用于确认用户身份的凭证。
6. Authentication：确定用户身份的过程。
7. Service：Openstack service，即Openstack中运行的组件服务。
8. Endpoint：一个可以通过网络来访问和定位某个Openstack service的地址，通常是一个URL。根据使用对象分为三类：
```shell
admin url –> 给admin用户使用，Post：35357
internal url –> OpenStack内部服务使用来跟别的服务通信，Port：5000
public url –> 其它用户可以访问的地址，Post：5000
```
8. domain，project，user，role之间的关系
9. ![](_v_images/20191026101840366_100014045.png)
8. keystone为openstack默认的认证服务，有三种认证方式：
  *  UUID
  *  PKI
  *  Fernet
#### 3.1.2 UUID认证方式
当用户需要访问nova，操作流程如下：
1. 首先要用有效的用户名/密码先去keystone认证，keystone返回给用户一个token(即UUID)。
2. 其次，用户使用获得的token向nova申请服务
3. nova收到请求后，用用户的token去向keystone进行请求验证。
4. keystone通过比对token，以及检查token的有效期，判断token的有效性，返回结果给nova。
5. nova根据keystone认证信息处理用户请求
  * yes， 发送200，给用户
  * no,  发送401,拒绝访问
    访问流程如下图所示：
![](_v_images/20191022021806598_1815744078.png)
#### 3.1.3 PKI认证
1.数字签名原理
![](_v_images/20191022100108093_1930434501.png)

2. 使用pki keystone认证流程
![](_v_images/20191022102113431_629719431.png)
#### 3.1.4 fernet认证
UUID方式的缺点：
 时间长了，会产出大量的token，导致数据库性能变差,改进：fernet
 fernet token
   当前主流推荐的token格式，它采用 cryptography 对称加密库(symmetric cryptography，加密密钥和解密密钥相同) 加密 token，具体由 AES-CBC 加密和散列函数 SHA256 签名。
不需要存储于数据库，减少了磁盘的 IO，带来了一定的性能提升。

### 3.2  前提条件（prerequisities)
1. 使用root用户连接数据库 
```shell
mysql -uroot -p
```
2. 创建keystone数据库
```shell
create database keystone;
```
3. 创建并授权keystone用户
```shell
  create user 'keystone'@'localhost' identified by 'keystone';
  create user 'keystone'@'%' identified by 'keystone';
  grant all privileges on keystone.* to keystone;
```
### 3.3 安装keystone，httpd及 mod_wsgi模块
1. openstack-keystone openstack认证软件
2. httpd apache http服务器
3. mod_wsgi模块，可以解析python软件
```shell
yum install openstack-keystone httpd mod_wsgi
```
4. 配置 /etc/keystone/keystone.conf文件
```shell
[database]
# ...
connection = mysql+pymysql://keystone:keystone@controller/keystone
[token]
# ...
provider = fernet
```
5. 同步数据库
```shell
# su -s /bin/sh -c "keystone-manage db_sync" keystone
```
**说明:**
 * su -s 后跟执行脚本 -s 是shell的缩写
 * sh -c  从字符串执行
 * sh -c ""  parameter , 作为字符串命令的第一个参数
 *  keystonemanage db_sync keystone
 5. 初始化fernet key 库
 ```shell
 # keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
#keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
 ```
 6. 创建bootstrap认证服务

 ```shell
 # keystone-manage bootstrap --bootstrap-password admin123 \
  --bootstrap-admin-url http://controller:5000/v3/ \
  --bootstrap-internal-url http://controller:5000/v3/ \
  --bootstrap-public-url http://controller:5000/v3/ \
  --bootstrap-region-id RegionOn
 ```
 7. 修改/etc/httpd/conf/httpd.conf文件
 ```shell
 ServerName controller
 ```
 8. 创建软连接文件

 ```shell
 ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
 ```
 9. 启动web 服务
 ```shell
# systemctl enable httpd.service
# systemctl start httpd.service
 ```
 10. 创建 admin环境变量文件，/root/keystone_admin
 ``` shell
export OS_USERNAME=admin
export OS_PASSWORD=admin123
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
 ```
 11. 输出环境变量
 ``` shell
 #source keystone_admin
 ```
###  3.4 domain，projcet，users and roles管理
#### 3.4.1 domain管理
1. 创建domain
```shell
#openstack domain create --description "An Example Domain" example
```
2. 查看domain
```shell
root@controlller ~]# openstack domain list
+----------------------------------+---------+---------+--------------------+
| ID                               | Name    | Enabled | Description        |
+----------------------------------+---------+---------+--------------------+
| b0bc02701f5348df9ddf727a2e99ecb2 | example | True    |  An Example Domain |
| default                          | Default | True    | The default domain |
+----------------------------------+---------+---------+--------------------+
```
3. 查看domain详细信息
```shell
root@controlller ~]# openstack domain show cloud1821
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | clound1821                       |
| enabled     | True                             |
| id          | b66b72e1feb145e38fad867a7cedeab6 |
| name        | cloud1821                        |
| tags        | []                               |
+-------------+----------------------------------+
```
4. 删除domain
要删除domain，首先要设置domain为disable，命令如下：
```shell
[root@controlller ~]# openstack domain set --disable example
```
然后执行删除命令：
```  shell
[root@controlller ~]# openstack domain delete example
[root@controlller ~]# openstack domain list
+---------+---------+---------+--------------------+
| ID      | Name    | Enabled | Description        |
+---------+---------+---------+--------------------+
| default | Default | True    | The default domain |
+---------+---------+---------+--------------------+
```
#### 3.4.2 project管理
1. 查看domain
```shell
[root@controlller ~]# openstack project list
+----------------------------------+-----------+
| ID                               | Name      |
+----------------------------------+-----------+
| 0fc81d875cca48e489028eb84146f7a4 | service2  |
| 28e2c67f8e16455a829ba18036490fd3 | myproject |
| 58e2a0860fe94bb4b244462fa9d39288 | admin     |
| 6ce019cfdf674c8392c3ee1af13d5d3d | service   |
+----------------------------------+-----------+
```
2. 创建project
 project必须属于某个domain
 * **首先要查看domain**
```shell
[root@controlller ~]# openstack domain list
+----------------------------------+-----------+---------+--------------------+
| ID                               | Name      | Enabled | Description        |
+----------------------------------+-----------+---------+--------------------+
| b66b72e1feb145e38fad867a7cedeab6 | cloud1821 | False   | clound1821         |
| default                          | Default   | True    | The default domain |
+----------------------------------+-----------+---------+--------------------+
```
 * **其次创建project**
```shell
[root@controlller ~]# openstack project create --domain default --description "create a demo domain"  demo
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | create a demo domain             |
| domain_id   | default                          |
| enabled     | True                             |
| id          | 5059700f003e42768de44d2b88fe1e57 |
| is_domain   | False                            |
| name        | demo                             |
| parent_id   | default                          |
| tags        | []                               |
+-------------+----------------------------------+
```
3. 删除project
```shell
[root@controlller ~]# openstack project list
+----------------------------------+-----------+
| ID                               | Name      |
+----------------------------------+-----------+
| 0fc81d875cca48e489028eb84146f7a4 | service2  |
| 28e2c67f8e16455a829ba18036490fd3 | myproject |
| 5059700f003e42768de44d2b88fe1e57 | demo      |
| 58e2a0860fe94bb4b244462fa9d39288 | admin     |
| 6ce019cfdf674c8392c3ee1af13d5d3d | service   |
+----------------------------------+-----------+
[root@controlller ~]# openstack project delete myproject
[root@controlller ~]# openstack project list
+----------------------------------+----------+
| ID                               | Name     |
+----------------------------------+----------+
| 0fc81d875cca48e489028eb84146f7a4 | service2 |
| 5059700f003e42768de44d2b88fe1e57 | demo     |
| 58e2a0860fe94bb4b244462fa9d39288 | admin    |
| 6ce019cfdf674c8392c3ee1af13d5d3d | service  |
+----------------------------------+----------+
```
4. 查看project详细信息
```shell
[root@controlller ~]# openstack project show demo
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | create a demo domain             |
| domain_id   | default                          |
| enabled     | True                             |
| id          | 5059700f003e42768de44d2b88fe1e57 |
| is_domain   | False                            |
| name        | demo                             |
| parent_id   | default                          |
| tags        | []                               |
+-------------+----------------------------------+
```
#### 3.4.3 user管理
1. 查看用户
```shell
[root@controlller ~]# openstack user list
+----------------------------------+--------+
| ID                               | Name   |
+----------------------------------+--------+
| 6fafe9a3635745ba879286c2d4e2edac | demo   |
| 9856f3f21b4d4e128d849d3b1c20a6c9 | user25 |
| e65881f9aa584883a3077c9ca481c566 | admin  |
+----------------------------------+--------+
```

2. 创建用户
用户创建必须属于某个domain
```shell
[root@controlller ~]# openstack user create --domain cloud1821 \
> --password-prompt user2
User Password:
Repeat User Password:
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | b66b72e1feb145e38fad867a7cedeab6 |
| enabled             | True                             |
| id                  | c57be863835e40138dc532197d845767 |
| name                | user2                            |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
```
3. 删除用户
```shell
root@controlller ~]# openstack user delete user2
[root@controlller ~]# openstack user list
+----------------------------------+--------+
| ID                               | Name   |
+----------------------------------+--------+
| 6fafe9a3635745ba879286c2d4e2edac | demo   |
| 9856f3f21b4d4e128d849d3b1c20a6c9 | user25 |
| e65881f9aa584883a3077c9ca481c566 | admin  |
+----------------------------------+--------+
```
4. 查看用户详细信息
```shell
[root@controlller ~]# openstack user show demo
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | default                          |
| enabled             | True                             |
| id                  | 6fafe9a3635745ba879286c2d4e2edac |
| name                | demo                             |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
```
5.  用户信息修改
语法：
```shell
openstack user set  options
```
options:

|名称|描述|
|-|-|
| --name <name>        | 设置用户名|
|    --domain <domain>     |设置domain|
|    --project <project>   |设置project|
|   --project-domain <project-domain> |设置project所属的domain|
|   --password <password> |设置密码|
|    --password-prompt     |设置密码输入提示|
|   --email <email-address> |设置邮件|
|   --description <description> |设置描述|
|   --enable             |设置用户起作用|
|  --disable             |设置用户不起作用|
**示例：**
更名：把user25 更名为user26
```shell
root@controlller ~]# openstack user show user25
+---------------------+----------------------------------+
| Field               | Value                            |
+---------------------+----------------------------------+
| domain_id           | b66b72e1feb145e38fad867a7cedeab6 |
| enabled             | True                             |
| id                  | 9856f3f21b4d4e128d849d3b1c20a6c9 |
| name                | user25                           |
| options             | {}                               |
| password_expires_at | None                             |
+---------------------+----------------------------------+
[root@controlller ~]# openstack user set --name user26  user25
[root@controlller ~]# openstack user list
+----------------------------------+--------+
| ID                               | Name   |
+----------------------------------+--------+
| 6fafe9a3635745ba879286c2d4e2edac | demo   |
| 9856f3f21b4d4e128d849d3b1c20a6c9 | user26 |
| e65881f9aa584883a3077c9ca481c566 | admin  |
+----------------------------------+--------+
```
更改user26的密码为000000
```shell
openstack user set --password '000000' user26
```
#### 3.4.4 role管理
1. 查看role
```shell
[root@controlller ~]# openstack role list
+----------------------------------+---------------+
| ID                               | Name          |
+----------------------------------+---------------+
| 3faf8c9b164b4751a7d6776c7cf13c54 | member        |
| 6dd0cba763b04c66b2b871c07517f10b | admin         |
| 753afe4fa2d94b55ac50cdc95372879a | reader        |
| b2686d3780274cf88441147afd2f4c8c | cloud1821role |
| b577c714b5284f2dab28c810c0b31fe4 | myrole        |
+----------------------------------+---------------+
```
2. 创建role
```shell
[root@controlller ~]# openstack role create myrole2
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | None                             |
| domain_id   | None                             |
| id          | de7e5e8fbe264997bcbb905b4efa923b |
| name        | myrole2                          |
+-------------+----------------------------------+
```
3. 修改role
可以修改domain，与名称
```shell
openstack role set [-h] [--domain <domain>] [--name <name>] <role>
```
4. 删除role
```shell
[root@controlller ~]# openstack role list
+----------------------------------+---------------+
| ID                               | Name          |
+----------------------------------+---------------+
| 3faf8c9b164b4751a7d6776c7cf13c54 | member        |
| 6dd0cba763b04c66b2b871c07517f10b | admin         |
| 753afe4fa2d94b55ac50cdc95372879a | reader        |
| b2686d3780274cf88441147afd2f4c8c | cloud1821role |
| b577c714b5284f2dab28c810c0b31fe4 | myrole        |
| de7e5e8fbe264997bcbb905b4efa923b | myrole2       |
+----------------------------------+---------------+
[root@controlller ~]# openstack role delete myrole2
[root@controlller ~]# openstack role list
+----------------------------------+---------------+
| ID                               | Name          |
+----------------------------------+---------------+
| 3faf8c9b164b4751a7d6776c7cf13c54 | member        |
| 6dd0cba763b04c66b2b871c07517f10b | admin         |
| 753afe4fa2d94b55ac50cdc95372879a | reader        |
| b2686d3780274cf88441147afd2f4c8c | cloud1821role |
| b577c714b5284f2dab28c810c0b31fe4 | myrole        |
+----------------------------------+---------------+
```
5. 增加add
添加可以添加project ,user
```shell
[root@controlller ~]# openstack role add --project service2 --user user26 myrole
```
6. 显示详细信息 show
```shell
[root@controlller ~]#openstack role show -f json myrole
{
  "id": "b577c714b5284f2dab28c810c0b31fe4",
  "description": null,
  "name": "myrole",
  "domain_id": null
}[root@controlller ~]#
```
### 3.5 获取token
#### 3.5.1使用openstack命令获取token
1. 编辑环境变量keystone_admin文件，内容如下：
```shell
[root@controlller ~]# cat keystone_admin
export OS_USERNAME=admin
export OS_PASSWORD=admin123
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
```
2. 输出环境变量:
```shell
 source  keystone_admin
```
3. 获取token
```shell
[root@controlller ~]# openstack token issue
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field      | Value                                                                                                                                                                                   |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| expires    | 2019-10-16T18:30:03+0000                                                                                                                                                                |
| id         | gAAAAABdp1Ob_DAf2oFIstLWFHWMgkiQGiDcXUc8ymFdIG5XUC4DN3v5VkJUHNqmsCWhLpkr3Eu--4ca5TR33qsWZUoISQjsO3lyCbmeWDJMJ_vLf10YD-yOwp-yld5FOgqALHG4r2oejAYhA6_9Ler42qFLrgOZyT1ZKOxtYdnFzq0q8FB9eUA |
| project_id | 58e2a0860fe94bb4b244462fa9d39288                                                                                                                                                        |
| user_id    | e65881f9aa584883a3077c9ca481c566                                                                                                                                                        |
+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
```
其中的id即为获取的token
#### 3.5.2 使用curl获取
1. curl
```shell
[root@controlller ~]# curl http://localhost:5000/v3/auth/tokens
{"error":{"code":401,"message":"The request you have made requires authentication.","title":"Unauthorized"}}
```
2. 必须进行授权才能够访问
3. 
```shell
[root@controlller ~]# (curl -i   -H "Content-Type: application/json"   -d '
{ "auth": {
    "identity": {
      "methods": ["password"],
      "password": {
        "user": {
          "name": "admin",
          "domain": { "id": "default" },
          "password": "admin123"
        }
      }
    },
    "scope": {
      "project": {
        "name": "admin",
        "domain": { "id": "default" }
      }
    }
  }
}'   "http://localhost:5000/v3/auth/tokens"; ) |grep "^X-Subject-Token:" | awk '{print $2}'
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1562  100  1220  100   342   1782    499 --:--:-- --:--:-- --:--:--  1781
gAAAAABdp1hb1YC1jTtenOq7YnQZfdSOd478_Y1wWBil4_Wa6i4VCD4b8JP_b9PVYSt8uATRMbs2Gcjg2GBvyb-abHlrgOrO3H8IxiS20az1sy7LOBkMuiUmWZHHzhusRFJ4kZe4vnCZU0JmiC-E7ksiMZoIn2D2hmcCfz2XJcvDIBGqjz2Ys0k
```
#### 3.5.3使用客户端获取
1. restclient firefox插件

   ![](_v_images\20191026114300124_563824183.png)

   ![20191026114917577_1495193117](_v_images\20191026114917577_1495193117.png)

   ![20191026115030955_2004563872](_v_images\20191026115030955_2004563872.png)

   ![20191026115139474_981542568](_v_images\20191026115139474_981542568.png)

   ![20191026115222783_1964047769](_v_images\20191026115222783_1964047769.png)

**备注：如果出现不可以访问现象，请在控制节点上执行：iptables -F**

# Glance服务:

>  在安装和配置展示位置服务之前，必须创建数据库，服务凭证和API端点。

#### 1. 创建数据库

1. 以`root`登陆数据库：

   ```shell
   mysql -u root -p
   ```

2. 创建`glance`数据库 ：

   ```shell
   MariaDB [(none)]> CREATE DATABASE glance;
   ```

3. 授予对**glance**数据库的适当访问权限：

   **注意：**`GLANCE_DBPASS`替换成自己的`glance`密码

   ```shell
   MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
     IDENTIFIED BY 'GLANCE_DBPASS';
   MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
     IDENTIFIED BY 'GLANCE_DBPASS';
   ```



#### 2. 来源admin凭据来访问仅管理员CLI命令：

```shell
source ./keystone-admin
```



#### 3.创建服务凭证：

1. 创建glance用户：

   **注意**：`User Password`就是glance的password

   ```shell
    root@controlller ~]$ openstack user create ‐‐description "glance user" glance
    No password was supplied, authentication will fail when a user does not have a password.
    +‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+
    | Field               | Value                            |
    +‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+
    | description         | glance user                      |
    | domain_id           | default                          |
    | enabled             | True                             |
    | id                  | 20f7c58057a248289ad71104d5f03abf |
    | name                | glance                           |
    | options             | {}                               |
    | password_expires_at | None                             |
    +‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐‐+
    [root@controlller ~]$ openstack user set ‐‐password‐prompt glance
    User Password:
    Repeat User Password:
   ```

2. 将`admin`角色添加到`glance`用户和 `service`项目：

   ```shell
   openstack role add --project service --user glance admin
   ```

3. 创建`glance`服务实体：

   ```shell
   openstack service create --name glance --description "OpenStack Image" image
   
   +-------------+----------------------------------+
   | Field       | Value                            |
   +-------------+----------------------------------+
   | description | OpenStack Image                  |
   | enabled     | True                             |
   | id          | 8c2c7f1b9b5049ea9e63757b5533e6d2 |
   | name        | glance                           |
   | type        | image                            |
   +-------------+----------------------------------+
   ```



####  4. 创建图像服务API端点：

1. 完成以下操作：

   ```shell
   $ openstack endpoint create --region RegionOne \
     image public http://controller:9292
   
   +--------------+----------------------------------+
   | Field        | Value                            |
   +--------------+----------------------------------+
   | enabled      | True                             |
   | id           | 340be3625e9b4239a6415d034e98aace |
   | interface    | public                           |
   | region       | RegionOne                        |
   | region_id    | RegionOne                        |
   | service_id   | 8c2c7f1b9b5049ea9e63757b5533e6d2 |
   | service_name | glance                           |
   | service_type | image                            |
   | url          | http://controller:9292           |
   +--------------+----------------------------------+
   
   $ openstack endpoint create --region RegionOne \
     image internal http://controller:9292
   
   +--------------+----------------------------------+
   | Field        | Value                            |
   +--------------+----------------------------------+
   | enabled      | True                             |
   | id           | a6e4b153c2ae4c919eccfdbb7dceb5d2 |
   | interface    | internal                         |
   | region       | RegionOne                        |
   | region_id    | RegionOne                        |
   | service_id   | 8c2c7f1b9b5049ea9e63757b5533e6d2 |
   | service_name | glance                           |
   | service_type | image                            |
   | url          | http://controller:9292           |
   +--------------+----------------------------------+
   
   $ openstack endpoint create --region RegionOne \
     image admin http://controller:9292
   
   +--------------+----------------------------------+
   | Field        | Value                            |
   +--------------+----------------------------------+
   | enabled      | True                             |
   | id           | 0c37ed58103f4300a84ff125a539032d |
   | interface    | admin                            |
   | region       | RegionOne                        |
   | region_id    | RegionOne                        |
   | service_id   | 8c2c7f1b9b5049ea9e63757b5533e6d2 |
   | service_name | glance                           |
   | service_type | image                            |
   | url          | http://controller:9292           |
   +--------------+----------------------------------+
   ```





## 安装和配置部份

#### 1.  安装软件包：

```shell
 yum install openstack-glance
```



#### 2. 编辑`/etc/glance/glance-api.conf`：

- 在该`[database]`部分中，配置数据库访问

  ```shell
  [database]
  # ...
  connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance
  ```

  **注意：**替换`GLANCE_DBPASS`为自己`glance`的密码

- 在`[keystone_authtoken]`和`[paste_deploy]`部分中，配置身份服务访问：

  ```shell
  [keystone_authtoken]
  # ...
  www_authenticate_uri  = http://controller:5000
  auth_url = http://controller:5000
  memcached_servers = controller:11211
  auth_type = password
  project_domain_name = Default
  user_domain_name = Default
  project_name = service
  username = glance
  password = GLANCE_PASS
  
  [paste_deploy]
  # ...
  flavor = keystone
  ```

  **注意：**替换`GLANCE_PASS`为自己`glance`的密码

- 在该`[glance_store]`部分中，配置本地文件系统存储和图像文件的位置：

  ```shell
  [glance_store]
  # ...
  stores = file,http
  default_store = file
  filesystem_store_datadir = /var/lib/glance/images/
  ```



#### 3. 编辑`/etc/glance/glance-registry.conf`：

- 在该`[database]`部分中，配置数据库访问：

  **注意：**替换`GLANCE_DBPASS`为自己`glance`的密码

  ```shell
  [database]
  # ...
  connection = mysql+pymysql://glance:GLANCE_DBPASS@controller/glance
  ```

- 在`[keystone_authtoken]`和`[paste_deploy]`部分中，配置身份服务访问：

  **注意：**替换`GLANCE_PASS`为自己`glance`的密码

  ```shell
  [keystone_authtoken]
  # ...
  www_authenticate_uri = http://controller:5000
  auth_url = http://controller:5000
  memcached_servers = controller:11211
  auth_type = password
  project_domain_name = Default
  user_domain_name = Default
  project_name = service
  username = glance
  password = GLANCE_PASS
  
  [paste_deploy]
  # ...
  flavor = keystone
  ```



#### 4. 同步glance数据库

```shell
su -s /bin/sh -c "glance-manage db_sync" glance
```



#### 5. 完成安装

- 设置开机自动启动，并启动glance

  ```shell
  $ systemctl enable openstack-glance-api.service \
    openstack-glance-registry.service
  $ systemctl start openstack-glance-api.service \
    openstack-glance-registry.service
  ```



## 验证

1. 查看9292端口是否可用

   ```shell
   [root@controller ~]$ ss -ntl | grep 9292
   LISTEN     0      128          *:9292                     *:*  
   ```

2. 来源`admin`凭据来访问仅管理员CLI命令：

   ```shell
   [root@controller ~]$ source keystone_admin
   ```

3. 上传镜像：

   ```shell
   [root@controller ~]$ openstack image create "cirros" \
   > --file /root/images/cirros-0.4.0-x86_64-disk.img \
   > --disk-format qcow2 --container-format bare \
   > --public
   
   +------------------+------------------------------------------------------+
   | Field            | Value                                                |
   +------------------+------------------------------------------------------+
   | checksum         | 133eae9fb1c98f45894a4e60d8736619                     |
   | container_format | bare                                                 |
   | created_at       | 2015-03-26T16:52:10Z                                 |
   | disk_format      | qcow2                                                |
   | file             | /v2/images/cc5c6982-4910-471e-b864-1098015901b5/file |
   | id               | cc5c6982-4910-471e-b864-1098015901b5                 |
   | min_disk         | 0                                                    |
   | min_ram          | 0                                                    |
   | name             | cirros                                               |
   | owner            | ae7a98326b9c455588edd2656d723b9d                     |
   | protected        | False                                                |
   | schema           | /v2/schemas/image                                    |
   | size             | 13200896                                             |
   | status           | active                                               |
   | tags             |                                                      |
   | updated_at       | 2015-03-26T16:52:10Z                                 |
   | virtual_size     | None                                                 |
   | visibility       | public                                               |
   +------------------+------------------------------------------------------+
   ```

4. 确认上传并验证属性：

   ```
   [root@controller ~]$ openstack image list
   +--------------------------------------+--------+--------+
   | ID                                   | Name   | Status |
   +------------------------------------a--+--------+--------+
   | 60c19af9-3980-4c23-b851-df8900d117f4 | cirros | active |
   +--------------------------------------+--------+--------+
   ```

   # Placement服务:

   >  在安装和配置展示位置服务之前，必须创建数据库，服务凭证和API端点。

   #### 1. 创建数据库

   1. 登陆数据库：

      ```shell
      [root@controller ~]$ mysql -u root -p
      ```

   2. 创建placement数据库：

      ```shell
      MariaDB [(none)]> CREATE DATABASE placement;shell
      ```

   3. 授予访问适当的权限： 

      **注意**： **PLACEMENT_DBPASS**： 使用自己的placement密码

      ```shell
      MariaDB [(none)]> GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' \
        IDENTIFIED BY 'PLACEMENT_DBPASS';
      MariaDB [(none)]> GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' \
        IDENTIFIED BY 'PLACEMENT_DBPASS';
      ```

   

   #### 2. 配置用户和端点

   1. 加载admin凭据来访问仅管理员CLI命令：

      ```shell
      [root@controller ~]$ source keystone_admin
      ```

   2. 创建placement 的user：

      ```shell
      [root@controller ~]$ openstack user create --domain default --password-prompt placement
      User Password:
      Repeat User Password:
      +---------------------+----------------------------------+
      | Field               | Value                            |
      +---------------------+----------------------------------+
      | domain_id           | default                          |
      | enabled             | True                             |
      | id                  | 54682aec469741c09d93f717bd5a34cf |
      | name                | placement                        |
      | options             | {}                               |
      | password_expires_at | None                             |
      +---------------------+----------------------------------+
      ```

   3. 将placement用户添加到服务器项目组中：

      ```shell
      [root@controller ~]$ openstack role add --project service --user placement admin
      ```

   4. 在服务目录中创建placement API项目：

      ```shell
      [root@controller ~]# openstack service create --name placement --description "Placement API" placement
      +-------------+----------------------------------+
      | Field       | Value                            |
      +-------------+----------------------------------+
      | description | Placement API                    |
      | enabled     | True                             |
      | id          | d2b846e769bb4fd291b4664c88cb4bb4 |
      | name        | placement                        |
      | type        | placement                        |
      +-------------+----------------------------------+
      ```

   5. 创建Placement API服务端点：

      > **注意**： 根据您的环境，终结点的URL将根据端口（可能是8780而不是8778，或根本没有端口）和主机名而有所不同。您有责任确定正确的URL。

      ```shell
      [root@controller ~]# openstack endpoint create --region RegionOne \
        placement public http://controller:8778
      
      +--------------+----------------------------------+
      | Field        | Value                            |
      +--------------+----------------------------------+
      | enabled      | True                             |
      | id           | 2b1b2637908b4137a9c2e0470487cbc0 |
      | interface    | public                           |
      | region       | RegionOne                        |
      | region_id    | RegionOne                        |
      | service_id   | 2d1a27022e6e4185b86adac4444c495f |
      | service_name | placement                        |
      | service_type | placement                        |
      | url          | http://controller:8778           |
      +--------------+----------------------------------+
      
      $ openstack endpoint create --region RegionOne \
        placement internal http://controller:8778
      
      +--------------+----------------------------------+
      | Field        | Value                            |
      +--------------+----------------------------------+
      | enabled      | True                             |
      | id           | 02bcda9a150a4bd7993ff4879df971ab |
      | interface    | internal                         |
      | region       | RegionOne                        |
      | region_id    | RegionOne                        |
      | service_id   | 2d1a27022e6e4185b86adac4444c495f |
      | service_name | placement                        |
      | service_type | placement                        |
      | url          | http://controller:8778           |
      +--------------+----------------------------------+
      
      $ openstack endpoint create --region RegionOne \
        placement admin http://controller:8778
      
      +--------------+----------------------------------+
      | Field        | Value                            |
      +--------------+----------------------------------+
      | enabled      | True                             |
      | id           | 3d71177b9e0f406f98cbff198d74b182 |
      | interface    | admin                            |
      | region       | RegionOne                        |
      | region_id    | RegionOne                        |
      | service_id   | 2d1a27022e6e4185b86adac4444c495f |
      | service_name | placement                        |
      | service_type | placement                        |
      | url          | http://controller:8778           |
      +--------------+----------------------------------+
      ```

   

   #### 3. 安装和配置部件

   1. 安装软件包：

      ```shell
      yum install openstack-placement-api
      ```

   2. 编辑`/etc/placement/placement.conf`文件并完成以下操作：

      - 在该`[placement_database]`部分中，配置数据库访问：

        ```shell
        [placement_database]
        # ...
        connection = mysql+pymysql://placement:PLACEMENT_DBPASS@controller/placement
        ```

        **注意**： **PLACEMENT_DBPASS** 使用自己的placement密码

      - 在`[api]`和`[keystone_authtoken]`部分中，配置身份服务访问：

        ```shell
        [api]
        # ...
        auth_strategy = keystone
        
        [keystone_authtoken]
        # ...
        auth_url = http://controller:5000/v3
        memcached_servers = controller:11211
        auth_type = password
        project_domain_name = Default
        user_domain_name = Default
        project_name = service
        username = placement
        password = PLACEMENT_PASS
        ```

        **注意：**PLACEMENT_PASS用自己的placement密码

   3. 同步placement数据库：

      ```shell
      su -s /bin/sh -c "placement-manage db sync" placement
      ```

   

   #### 4.完成安装

   1. 重启`httpd`服务

      ```shell
      systemctl restart httpd
      ```

   

   #### 5. 验证

   1. 来源`admin`凭据来访问仅管理员CLI命令：

      ```
      $ . admin-openrc
      ```

   2. 执行状态检查以确保一切正常：

      ```
      $ placement-status upgrade check
      +----------------------------------+
      | Upgrade Check Results            |
      +----------------------------------+
      | Check: Missing Root Provider IDs |
      | Result: Success                  |
      | Details: None                    |
      +----------------------------------+
      | Check: Incomplete Consumers      |
      | Result: Success                  |
      | Details: None                    |
      +----------------------------------+
      ```

   3. 执行状态检查以确保一切正常：

      ```shell
      1. 1. $ placement-status upgrade check
            +----------------------------------+
            | Upgrade Check Results         |
            +----------------------------------+
            | Check: Missing Root Provider IDs |
            | Result: Success             |
            | Details: None              |
            +----------------------------------+
            | Check: Incomplete Consumers     |
            | Result: Success             |
            | Details: None              |
            +----------------------------------+|
      ```
      



# nova服务

## nova服务控制节点

> 在安装和配置Compute服务之前，必须创建**数据库**，**服务凭证**和**API端点**。

#### 1. 创建数据库

- 进入数据库

  ```shell
  $ mysql -uroot -p
  ```

- 创建`nova_api`，`nova`和`nova_cell0`数据库：

  ```shell
  MariaDB [(none)]> CREATE DATABASE nova_api;
  MariaDB [(none)]> CREATE DATABASE nova;
  MariaDB [(none)]> CREATE DATABASE nova_cell0;
  ```

- 授予对数据库的访问权限：

  ```shell
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \
    IDENTIFIED BY 'NOVA_DBPASS';
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \
    IDENTIFIED BY 'NOVA_DBPASS';
  
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
    IDENTIFIED BY 'NOVA_DBPASS';
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
    IDENTIFIED BY 'NOVA_DBPASS';
  
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \
    IDENTIFIED BY 'NOVA_DBPASS';
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \
    IDENTIFIED BY 'NOVA_DBPASS';
  ```

  > **注意：** NOVA_DBPASS使用自己的nova数据库密码

- 退出数据库



#### 2. 加载管理员权限：

```shell
$ . keystone_admin
```



#### 3. 创建计算服务凭据

- 创建nova用户：

  ```shell
  $  openstack user create --domain default --password-prompt nova
  
  User Password:
  Repeat User Password:
  +---------------------+----------------------------------+
  | Field               | Value               |
  +---------------------+----------------------------------+
  | domain_id           | default               |
  | enabled            | True                |
  | id                | 8a7dbf5279404537b1c7b86c033620fe |
  | name               | nova                   |
  | options             | {}                  |
  | password_expires_at | None                    |
  +---------------------+----------------------------------+
  ```

- 把nova添加到admin中： 

  ```shell
  $ openstack role add --project service --user nova admin
  ```

- 创建nova服务：

  ```shell
  $ openstack service create --name nova \
    --description "OpenStack Compute" compute
  
  +-------------+----------------------------------+
  | Field       | Value                 |
  +-------------+----------------------------------+
  | description | OpenStack Compute           |
  | enabled    | True                    |
  | id       | 060d59eac51b4594815603d75a00aba2 |
  | name      | nova                    |
  | type      | compute                  |
  +-------------+----------------------------------+
  ```



#### 4. 创建Computer API服务端点：

```shell
$ openstack endpoint create --region RegionOne \
  compute public http://controller:8774/v2.1

+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled    | True                                      |
| id        | 3c1caa473bfe4390a11e7177894bcc7b          |
| interface    | public                                    |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 060d59eac51b4594815603d75a00aba2          |
| service_name | nova                                      |
| service_type | compute                                   |
| url          | http://controller:8774/v2.1               |
+--------------+-------------------------------------------+

$ openstack endpoint create --region RegionOne \
  compute internal http://controller:8774/v2.1

+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | e3c918de680746a586eac1f2d9bc10ab          |
| interface    | internal                                  |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 060d59eac51b4594815603d75a00aba2          |
| service_name | nova                                      |
| service_type | compute                                   |
| url          | http://controller:8774/v2.1               |
+--------------+-------------------------------------------+

$ openstack endpoint create --region RegionOne \
  compute admin http://controller:8774/v2.1

+--------------+-------------------------------------------+
| Field        | Value                                     |
+--------------+-------------------------------------------+
| enabled      | True                                      |
| id           | 38f7af91666a47cfb97b4dc790b94424          |
| interface    | admin                                     |
| region       | RegionOne                                 |
| region_id    | RegionOne                                 |
| service_id   | 060d59eac51b4594815603d75a00aba2          |
| service_name | nova                                      |
| service_type | compute                                   |
| url          | http://controller:8774/v2.1               |
+--------------+-------------------------------------------+
```

#### 1. 安装软件包

```shell
$ yum install openstack-nova-api openstack-nova-conductor \
  openstack-nova-novncproxy openstack-nova-scheduler
```



#### 2.配置`/etc/nova/nova.conf`文件并完成以下操作:

- 在`[DEFAULT]`部分中，仅启用计算和元数据API：

  ```shell
  [DEFAULT]
  enabled_apis = osapi_compute,metadata
  ```

- 在`[api_database]`和`[database]`部分中，配置数据库访问权限：

  ```shell
  [api_database]
  connection = mysql+pymysql://nova:NOVA_DBPASS@controller/nova_api
  ```

  替换`NOVA_DBPASS`为您为Compute数据库选择的密码。

- 在该`[DEFAULT]`部分中，配置`RabbitMQ`消息队列访问：

  ```shell
  transport_url = rabbit://openstack:RABBIT_PASS@controller
  ```

  替换`RABBIT_PASS`为您在中为`openstack` 帐户选择的密码`RabbitMQ`。

- 在`[api]`和`[keystone_authtoken]`部分中，配置身份服务访问：

  ```shell
  [api]
  auth_strategy = keystone
  
  [keystone_authtoken]
  # ...
  auth_url = http://controller:5000/v3
  memcached_servers = controller:11211
  auth_type = password
  project_domain_name = Default
  user_domain_name = Default
  project_name = service
  username = nova
  password = NOVA_PASS
  ```

  替换`NOVA_PASS`为您`nova`在身份服务中为用户选择的密码。

  > 注意
  >
  > 注释掉或删除此`[keystone_authtoken]` 部分中的任何其他选项。

- 在该`[DEFAULT]`部分中，配置`my_ip`选项以使用控制器节点的管理接口IP地址：

  ```shell
  [DEFAULT]
  
  my_ip = 192.168.10.17
  ```

- 在本`[DEFAULT]`节中，启用对网络服务的支持：

  ```shell
  [DEFAULT]
  
  use_neutron = true
  firewall_driver = nova.virt.firewall.NoopFirewallDriver
  ```

  > 注意：
  >
  > 默认情况下，Compute使用内部防火墙驱动程序。由于网络服务包含防火墙驱动程序，因此必须使用`nova.virt.firewall.NoopFirewallDriver`防火墙驱动程序禁用计算防火墙驱动 程序。

- 在该`[vnc]`部分中，将VNC代理配置为使用控制器节点的管理接口IP地址：

  ```shell
  [vnc]
  enabled = true
  # ...
  server_listen = $my_ip
  server_proxyclient_address = $my_ip
  ```

- 在该`[glance]`部分中，配置图像服务API的位置：

  ```
  [glance]
  # ...
  api_servers = http://controller:9292
  ```

- 在该`[oslo_concurrency]`部分中，配置锁定路径：

  ```
  [oslo_concurrency]
  # ...
  lock_path = /var/lib/nova/tmp
  ```

- 在该`[placement]`部分中，配置对展示位置服务的访问权限：

  ```
  [placement]
  # ...
  region_name = RegionOne
  project_domain_name = Default
  project_name = service
  auth_type = password
  user_domain_name = Default
  auth_url = http://controller:5000/v3
  username = placement
  password = PLACEMENT_PASS
  ```

  替换`PLACEMENT_PASS`为您为`placement`安装[Placement](https://docs.openstack.org/placement/stein/install/)时创建的服务用户 选择的密码 。注释掉或删除此`[placement]`部分中的任何其他选项。



#### 3. 填充nova-api数据库：

```shell
$  su -s /bin/sh -c "nova-manage api_db sync" nova
```

>  **注意：**
>
>  忽略此输出中的所有弃用消息。



#### 4. 注册cell0数据库

```shell
$ su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
```



#### 5.创建cell1单元格：

```she
$ su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova
109e1d4b-536a-40d0-83c6-5f121b82b650
```



#### 6. 填充nova数据库：

```shell
$ su -s /bin/sh -c "nova-manage db sync" nova
```



#### 7. 验证nova cell0和cell1是否正确注册：

```shell
$ su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova
+-------+--------------------------------------+
| Name  | UUID                                 |
+-------+--------------------------------------+
| cell1 | 109e1d4b-536a-40d0-83c6-5f121b82b650 |
| cell0 | 00000000-0000-0000-0000-000000000000 |
+-------+--------------------------------------+
```



## 确定安装

- 启动Compute服务并将其配置为在系统启动时启动：

  ```shell
  $ systemctl enable openstack-nova-api.service \
    openstack-nova-consoleauth openstack-nova-scheduler.service \
    openstack-nova-conductor.service openstack-nova-novncproxy.service
  
  $ systemctl start openstack-nova-api.service \
    openstack-nova-consoleauth openstack-nova-scheduler.service \
    openstack-nova-conductor.service openstack-nova-novncproxy.service
  ```

  

## nova服务计算节点

1. 安装软件包：

   ```SAS
   # yum install openstack-nova-compute
   ```

2. 编辑`/etc/nova/nova.conf`文件并完成以下操作：

   - 在此`[DEFAULT]`部分中，仅启用计算和元数据API：

     ```
     [DEFAULT]
     # ...
     enabled_apis = osapi_compute,metadata
     ```

   - 在该`[DEFAULT]`部分中，配置`RabbitMQ`消息队列访问：

     ```
     [DEFAULT]
     # ...
     transport_url = rabbit://openstack:RABBIT_PASS@controller
     ```

     替换`RABBIT_PASS`为您在中为`openstack` 帐户选择的密码`RabbitMQ`。

   - 在`[api]`和`[keystone_authtoken]`部分中，配置身份服务访问：

     ```shell
     [api]
     # ...
     auth_strategy = keystone
     
     [keystone_authtoken]
     # ...
     auth_url = http://controller:5000/v3
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = Default
     user_domain_name = Default
     project_name = service
     username = nova
     password = NOVA_PASS
     ```

     替换`NOVA_PASS`为您`nova`在身份服务中为用户选择的密码。

     > **注意**
     >
     > 注释掉或删除此`[keystone_authtoken]` 部分中的任何其他选项。

   - 在该`[DEFAULT]`部分中，配置`my_ip`选项：

     ```shell
     [DEFAULT]
     # ...
     my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
     ```

     替换`MANAGEMENT_INTERFACE_IP_ADDRESS`为计算节点上管理网络接口的IP地址。

   - 在本`[DEFAULT]`节中，启用对网络服务的支持：

     ```shell
     [DEFAULT]
     # ...
     use_neutron = true
     firewall_driver = nova.virt.firewall.NoopFirewallDriver
     ```

     > 注意
     >
     > 默认情况下，Compute使用内部防火墙服务。由于联网包含防火墙服务，因此必须使用`nova.virt.firewall.NoopFirewallDriver`防火墙驱动程序禁用计算防火墙服务。

   - 在该`[vnc]`部分中，启用和配置远程控制台访问：

     ```shell
     [vnc]
     # ...
     enabled = true
     server_listen = 0.0.0.0
     server_proxyclient_address = $my_ip
     novncproxy_base_url = http://controller:6080/vnc_auto.html
     ```

     服务器组件侦听所有IP地址，代理组件侦听所有计算节点的管理接口IP地址。基本URL指示可以使用Web浏览器访问此计算节点上的实例的远程控制台的位置。

     >  注意
     >
     >  如果用于访问远程控制台的Web浏览器驻留在无法解析`controller`主机名的主机上，则必须替换 `controller`为控制器节点的管理接口IP地址。

   - 在该`[glance]`部分中，配置图像服务API的位置：

     ```shell
     [glance]
     # ...
     api_servers = http://controller:9292
     ```

   - 在该`[oslo_concurrency]`部分中，配置锁定路径：

     ```shell
     [oslo_concurrency]
     # ...
     lock_path = /var/lib/nova/tmp
     ```

   - 在该`[placement]`部分中，配置Placement API：

     ```shell
     [placement]
     # ...
     region_name = RegionOne
     project_domain_name = Default
     project_name = service
     auth_type = password
     user_domain_name = Default
     auth_url = http://controller:5000/v3
     username = placement
     password = PLACEMENT_PASS
     ```

     替换`PLACEMENT_PASS`为您`placement`在身份服务中为用户选择的密码 。注释掉该`[placement]`部分中的所有其他选项。

     

   ## 最终确定安装

   1. 确定您的计算节点是否支持虚拟机的硬件加速：

      ```shell
      $ egrep -c '(vmx|svm)' /proc/cpuinfo
      ```

      如果此命令返回值，则您的计算节点支持硬件加速，通常不需要其他配置。`one or greater`

      如果此命令返回值`zero`，则您的计算节点不支持硬件加速，并且您必须配置`libvirt`为使用QEMU而不是KVM。

   2. 编辑文件中的`[libvirt]`部分，`/etc/nova/nova.conf`如下所示：

      ```shell
      [libvirt]
      # ...
      virt_type = qemu
      ```

   3. 启动Compute服务及其相关性，并将其配置为在系统启动时自动启动：

      ```shell
      $ systemctl enable libvirtd.service openstack-nova-compute.service
      $ systemctl start libvirtd.service openstack-nova-compute.service 
      ```

   > 注意
   >
   > 如果`nova-compute`服务无法启动，请检查 `/var/log/nova/nova-compute.log`。该错误消息可能表明控制器节点上的防火墙阻止访问端口5672。将防火墙配置为打开控制器节点上的端口5672并重新启动 计算节点上的服务。`AMQP server on controller:5672 is unreachable``nova-compute`

   

   ## 将计算节点添加到单元数据库中

   >  **重要**
   >
   >  在**控制节点**上运行以下命令。

   1. 获取管理员凭据以启用仅管理员的CLI命令，然后确认数据库中有计算主机：

      ```shell
      $ . admin-openrc
      
      $ openstack compute service list --service nova-compute
      +----+-------+--------------+------+-------+---------+----------------------------+
      | ID | Host  | Binary       | Zone | State | Status  | Updated At                 |
      +----+-------+--------------+------+-------+---------+----------------------------+
      | 1  | node1 | nova-compute | nova | up    | enabled | 2017-04-14T15:30:44.000000 |
      +----+-------+--------------+------+-------+---------+----------------------------+
      ```

   2. 发现计算主机：

      ```shell
      $ su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
      
      Found 2 cell mappings.
      Skipping cell0 since it does not contain hosts.
      Getting compute nodes from cell 'cell1': ad5a5985-a719-4567-98d8-8d148aaae4bc
      Found 1 computes in cell: ad5a5985-a719-4567-98d8-8d148aaae4bc
      Checking host mapping for compute host 'compute': fe58ddc1-1d65-4f87-9456-bc040dc106b3
      Creating host mapping for compute host 'compute': fe58ddc1-1d65-4f87-9456-bc040dc106b3
      ```

      >  注意
      >
      >  添加新的计算节点时，必须在控制节点上运行以注册这些新的计算节点。另外，您可以在中设置适当的时间间隔 ：
      >
      >  `nova-manage cell_v2 discover_hosts``/etc/nova/nova.conf`

      - 编辑`/etc/nova/nova.conf`

        ```shell
        [scheduler]
        discover_hosts_in_cells_interval = 300
        ```



## 验证计算nova服务的运行

> **注意：**
>
> 在**控制节点**上执行这些命令

1. 来源`admin`凭据来访问仅管理员CLI命令：

   ```
   $ . admin-openrc
   ```

   

2. 列出服务组件以验证每个进程是否成功启动和注册：

   ```
   $ openstack compute service list
   
   +----+--------------------+------------+----------+---------+-------+----------------------------+
   | Id | Binary             | Host       | Zone     | Status  | State | Updated At                 |
   +----+--------------------+------------+----------+---------+-------+----------------------------+
   |  1 | nova-scheduler     | controller | internal | enabled | up    | 2016-02-09T23:11:15.000000 |
   |  2 | nova-conductor     | controller | internal | enabled | up    | 2016-02-09T23:11:16.000000 |
   |  3 | nova-compute       | compute1   | nova     | enabled | up    | 2016-02-09T23:11:20.000000 |
   +----+--------------------+------------+----------+---------+-------+----------------------------+
   ```

   > **注意：**
   >
   > 此输出应指示在控制器节点上启用的两个服务组件和在计算节点上启用的一个服务组件。

   

3. 列出身份服务中的API端点以验证与身份服务的连接性：

   > **注意：**
   >
   > 以下端点列表可能会有所不同，具体取决于OpenStack组件的安装。

   ```shell
   $ openstack catalog list
   
   +-----------+-----------+-----------------------------------------+
   | Name      | Type      | Endpoints                               |
   +-----------+-----------+-----------------------------------------+
   | keystone  | identity  | RegionOne                               |
   |           |           |   public: http://controller:5000/v3/    |
   |           |           | RegionOne                               |
   |           |           |   internal: http://controller:5000/v3/  |
   |           |           | RegionOne                               |
   |           |           |   admin: http://controller:5000/v3/     |
   |           |           |                                         |
   | glance    | image     | RegionOne                               |
   |           |           |   admin: http://controller:9292         |
   |           |           | RegionOne                               |
   |           |           |   public: http://controller:9292        |
   |           |           | RegionOne                               |
   |           |           |   internal: http://controller:9292      |
   |           |           |                                         |
   | nova      | compute   | RegionOne                               |
   |           |           |   admin: http://controller:8774/v2.1    |
   |           |           | RegionOne                               |
   |           |           |   internal: http://controller:8774/v2.1 |
   |           |           | RegionOne                               |
   |           |           |   public: http://controller:8774/v2.1   |
   |           |           |                                         |
   | placement | placement | RegionOne                               |
   |           |           |   public: http://controller:8778        |
   |           |           | RegionOne                               |
   |           |           |   admin: http://controller:8778         |
   |           |           | RegionOne                               |
   |           |           |   internal: http://controller:8778      |
   |           |           |                                         |
   +-----------+-----------+-----------------------------------------+
   ```

   > **注意：**
   >
   > 忽略此输出中的任何警告。

   

4. 列出图像服务中的图像以验证与图像服务的连接性：

   ```shell
   $ openstack image list
   
   +--------------------------------------+-------------+-------------+
   | ID                                   | Name        | Status      |
   +--------------------------------------+-------------+-------------+
   | 9a76d9f9-9620-4f2e-8c69-6c5691fae163 | cirros      | active      |
   +--------------------------------------+-------------+-------------+
   ```

   

5. 检查单元格和展示位置API是否正常运行，以及其他必要的前提条件是否到位：

   ```
   $ nova-status upgrade check
   
   +--------------------------------------------------------------------+
   | Upgrade Check Results                                              |
   +--------------------------------------------------------------------+
   | Check: Cells v2                                                    |
   | Result: Success                                                    |
   | Details: None                                                      |
   +--------------------------------------------------------------------+
   | Check: Placement API                                               |
   | Result: Success                                                    |
   | Details: None                                                      |
   +--------------------------------------------------------------------+
   | Check: Ironic Flavor Migration                                     |
   | Result: Success                                                    |
   | Details: None                                                      |
   +--------------------------------------------------------------------+
   | Check: Request Spec Migration                                      |
   | Result: Success                                                    |
   | Details: None                                                      |
   +--------------------------------------------------------------------+
   | Check: Console Auths                                               |
   | Result: Success                                                    |
   | Details: None                                                      |
   +--------------------------------------------------------------------+
   ```

   > **注意：**
   >
   > **如果`nova-status upgrade check`出现403错误**则执行以下操作可解决：

   - 修改配置文件`/etc/httpd/conf.d/00-nova-placement-api.conf`并重启`httpd

   -  

     ```
     <Directory /usr/bin>
         <IfVersion >= 2.4>
            Require all granted
         </IfVersion>
         <IfVersion < 2.4>
            Order allow,deny
            Allow from all
         </IfVersion>
      </Directory>
     
     # 重启
     ```

     ```
      systemctl restart httpd
     ```

     



# Neutron服务配置

## 控制节点

#### 1.创建数据库

- 使用数据库访问客户端以`root`用户身份连接到数据库服务器： 

  ```shell
  $ mysql -u root -p
  ```

- 创建`neutron`数据库： 

  ```shell
  MariaDB [(none)] CREATE DATABASE neutron;
  ```

- 授予对`neutron`数据库的适当访问权限，`NEUTRON_DBPASS`并用合适的密码代替 ： 

  ```shell
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
    IDENTIFIED BY 'NEUTRON_DBPASS';
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
    IDENTIFIED BY 'NEUTRON_DBPASS';
  ```

- 退出数据库访问客户端。 

#### 2. 来源`admin`凭据来访问仅管理员CLI命令：

```shell
$ . admin-openrc
```

#### 3. 要创建服务凭证，请完成以下步骤 

- 创建`neutron`用户： 

  ```shell
  $ openstack user create --domain default --password-prompt neutron
  
  User Password:
  Repeat User Password:
  +---------------------+----------------------------------+
  | Field          | Value                            |
  +---------------------+----------------------------------+
  | domain_id        | default                          |
  | enabled         | True                            |
  | id            | fdb0f541e28141719b6a43c8944bf1fb          |
  | name           | neutron                          |
  | options         | {}                             |
  | password_expires_at | None                            |
  +---------------------+----------------------------------+
  ```

- `admin`向`neutron`用户添加角色：

  ```shell
  $ openstack role add --project service --user neutron admin
  ```

  >  **注意**:该命令不提供任何输出。

- 创建`neutron`服务实体： 

  ```shell
  openstack service create --name neutron \
    --description "OpenStack Networking" network
    
  +-------------+----------------------------------+
  | Field       | Value                         |
  +-------------+----------------------------------+
  | description | OpenStack Networking                 |
  | enabled    | True                            |
  | id       | f71529314dab4a4d8eca427e701d209e         |
  | name      | neutron                          |
  | type      | network                          |
  +-------------+----------------------------------+
  ```



#### 4. 创建网络服务API端点： 

```shell
$ openstack endpoint create --region RegionOne \
  network public http://controller:9696

+--------------+----------------------------------+
| Field        | Value                            |
+--------------+----------------------------------+
| enabled      | True                             |
| id           | 85d80a6d02fc4b7683f611d7fc1493a3 |
| interface    | public                          |
| region       | RegionOne                       |
| region_id    | RegionOne                        |
| service_id   | f71529314dab4a4d8eca427e701d209e |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller:9696             |
+--------------+----------------------------------+

$ openstack endpoint create --region RegionOne \
  network internal http://controller:9696

+--------------+----------------------------------+
| Field        | Value                         |
+--------------+----------------------------------+
| enabled      | True                          |
| id           | 09753b537ac74422a68d2d791cf3714f     |
| interface    | internal                        |
| region       | RegionOne                      |
| region_id    | RegionOne                        |
| service_id   | f71529314dab4a4d8eca427e701d209e        |
| service_name | neutron                          |
| service_type | network                          |
| url          | http://controller:9696             |
+--------------+----------------------------------+

$ openstack endpoint create --region RegionOne \
  network admin http://controller:9696

+--------------+----------------------------------+
| Field      | Value                            |
+--------------+----------------------------------+
| enabled     | True                            |
| id        | 1ee14289c9374dffb5db92a5c112fc4e |
| interface    | admin                            |
| region      | RegionOne                         |
| region_id    | RegionOne                         |
| service_id   | f71529314dab4a4d8eca427e701d209e          |
| service_name  | neutron                          |
| service_type  | network                          |
| url        | http://controller:9696                |
+--------------+----------------------------------+
```

### 配置网络

```shell
  yum install openstack-neutron openstack-neutron-ml2 \
  openstack-neutron-linuxbridge ebtables
```

### 配置服务器组件

- 编辑`/etc/neutron/neutron.conf`文件并完成以下操作： 

  - 在该`[database]`部分中，配置数据库访问： 

    ```shell
    [database]
    # ...
    connection = mysql+pymysql://neutron:NEUTRON_DBPASS@controller/neutron
    ```

     替换`NEUTRON_DBPASS`为您为数据库选择的密码。 

  - 在该`[DEFAULT]`部分中，启用模块化第2层（ML2）插件，路由器服务和重叠的IP地址： 

    ```shell
    [DEFAULT]
    # ...
    core_plugin = ml2
    service_plugins = router
    allow_overlapping_ips = true
    ```

  - 在该`[DEFAULT]`部分中，配置`RabbitMQ` 消息队列访问： 

    ```shell
    [DEFAULT]
    # ...
    transport_url = rabbit://openstack:RABBIT_PASS@controller
    ```

     替换`RABBIT_PASS`为您`openstack`在RabbitMQ中为帐户选择的密码 。 

  - 在`[DEFAULT]`和`[keystone_authtoken]`部分中，配置身份服务访问：

    ```shell
    [DEFAULT]
    # ...
    auth_strategy = keystone
    
    [keystone_authtoken]
    # ...
    www_authenticate_uri = http://controller:5000
    auth_url = http://controller:5000
    memcached_servers = controller:11211
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    project_name = service
    username = neutron
    password = NEUTRON_PASS
    ```

     替换`NEUTRON_PASS`为您`neutron` 在身份服务中为用户选择的密码。 

  - 在`[DEFAULT]`和`[nova]`部分中，将网络配置为通知Compute网络拓扑更改：

    ```shell
    [DEFAULT]
    # ...
    notify_nova_on_port_status_changes = true
    notify_nova_on_port_data_changes = true
    
    [nova]
    # ...
    auth_url = http://controller:5000
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    region_name = RegionOne
    project_name = service
    username = nova
    password = NOVA_PASS
    ```

     替换`NOVA_PASS`为您`nova` 在身份服务中为用户选择的密码。 

  - 在该`[oslo_concurrency]`部分中，配置锁定路径： 

    ```shell
    [oslo_concurrency]
    # ...
    lock_path = /var/lib/neutron/tmp
    ```

 替换`NEUTRON_DBPASS`为您为数据库选择的密码。 

### 配置模块化层2（ML2）插件

 ML2插件使用Linux桥接器机制为实例构建第2层（桥接和交换）虚拟网络基础架构。 

- 编辑`/etc/neutron/plugins/ml2/ml2_conf.ini`文件并完成以下操作： 

  - 在该`[ml2]`部分中，启用平面，VLAN和VXLAN网络： 

    ```shell
    [ml2]
    # ...
    type_drivers = flat,vlan,vxlan
    ```

  - 在该`[ml2]`部分中，启用VXLAN自助服务网络： 

    ```shell
    [ml2]
    # ...
    tenant_network_types = vxlan
    ```

  - 在本`[ml2]`节中，启用Linux桥接器和第2层填充机制： 

    ```shell
    [ml2]
    # ...
    mechanism_drivers = linuxbridge,l2population
    ```

  - 在此`[ml2]`部分中，启用端口安全扩展驱动程序：

    ```shell
    [ml2]
    # ...
    extension_drivers = port_security
    ```

  - 在本`[ml2_type_flat]`节中，将提供者虚拟网络配置为平面网络：

    ```shell
    [ml2_type_flat]
    # ...
    flat_networks = provider
    ```

  - 在该`[ml2_type_vxlan]`部分中，为自助服务网络配置VXLAN网络标识符范围：

    ```shell
    [ml2_type_vxlan]
    # ...
    vni_ranges = 1:1000
    ```

  - 在本`[securitygroup]`节中，启用ipset以提高安全组规则的效率： 

    ```shell
    [securitygroup]
    # ...
    enable_ipset = true
    ```

### 配置Linux网桥代理

Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础结构并处理安全组。

- 编辑`/etc/neutron/plugins/ml2/linuxbridge_agent.ini`文件并完成以下操作： 

  - 在本`[linux_bridge]`节中，将提供者虚拟网络映射到提供者物理网络接口：

    ```shell
    [linux_bridge]
    physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME
    ```

     替换`PROVIDER_INTERFACE_NAME`为基础提供商物理网络接口的名称能上网的接口.例如：ens33

  - 在本`[vxlan]`节中，启用VXLAN覆盖网络，配置处理覆盖网络的物理网络接口的IP地址，并启用第2层填充： 

    ```shell
    [vxlan]
    enable_vxlan = true
    local_ip = OVERLAY_INTERFACE_IP_ADDRESS
    l2_population = true
    ```

     替换**OVERLAY_INTERFACE_IP_ADDRESS* 为控制器节点的管理IP地址 

  - 在该`[securitygroup]`部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序： 

    ```shell
    [securitygroup]
    # ...
    enable_security_group = true
    firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
    ```

  - 通过验证以下所有`sysctl`值是否设置为确保Linux操作系统内核支持网桥过滤器`1`

  - vi /etc/sysctl.conf：增加以下两行后输入命令 sysctl -p

    ```shell
    net.bridge.bridge-nf-call-iptables=1
    net.bridge.bridge-nf-call-ip6tables=1
    ```

     

### 配置第3层剂

第3层（L3）代理为自助式虚拟网络提供路由和NAT服务。

- 编辑`/etc/neutron/l3_agent.ini`文件并完成以下操作： 

  - 在该`[DEFAULT]`部分中，配置Linux网桥接口驱动程序和外部网桥： 

    ```shell
    [DEFAULT]
    # ...
    interface_driver = linuxbridge
    ```

### 配置DHCP代理

DHCP代理为虚拟网络提供DHCP服务。

- 编辑`/etc/neutron/dhcp_agent.ini`文件并完成以下操作： 

  - 在本`[DEFAULT]`节中，配置Linux桥接口驱动程序Dnsmasq DHCP驱动程序，并启用隔离的元数据，以便提供商网络上的实例可以通过网络访问元数据： 

    ```shell
    [DEFAULT]
    # ...
    interface_driver = linuxbridge
    dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
    enable_isolated_metadata = true
    ```

### 配置元数据代理

#####  元数据代理提供配置信息，例如实例的凭据。 

- 编辑`/etc/neutron/metadata_agent.ini`文件并完成以下操作： 

  - 在该**[DEFAULT]**部分中，配置元数据主机和共享机密：

    

    ```shell
    [DEFAULT]
    # ...
    nova_metadata_host = controller
    metadata_proxy_shared_secret = METADATA_SECRET
    ```

     替换**METADATA_SECRET**为元数据代理的适当机密。 例如：123456

### 配置计算服务使用网络服务

> 必须安装Nova计算服务才能完成此步骤。

- 编辑`/etc/nova/nova.conf`文件并执行以下操作：

  - 在该`[neutron]`部分中，配置访问参数，启用元数据代理，并配置机密：

    ```shell
    [neutron]
    # ...
    url = http://controller:9696
    auth_url = http://controller:5000
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    region_name = RegionOne
    project_name = service
    username = neutron
    password = NEUTRON_PASS
    service_metadata_proxy = true
    metadata_proxy_shared_secret = METADATA_SECRET
    ```

    替换**NEUTRON_PASS**为您`neutron` 在身份服务中为用户选择的密码。

    替换**METADATA_SECRET**为您为元数据代理选择的机密。

    

### 最终确定安装

1. 网络服务初始化脚本需要`/etc/neutron/plugin.ini`指向ML2插件配置文件的符号链接`/etc/neutron/plugins/ml2/ml2_conf.ini`。如果此符号链接不存在，请使用以下命令创建它： 

   ```shell
   ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
   ```

2. 填充数据库： 

   ```shell
   su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
     --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
   ```

3. 重新启动Compute API服务：

   ```shell
   systemctl restart openstack-nova-api.service
   ```

   

4. 启动网络服务，并将其配置为在系统引导时启动。

   对于两个网络选项： 

   ```shell
   $ systemctl enable neutron-server.service \
     neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
     neutron-metadata-agent.service
   $ systemctl start neutron-server.service \
     neutron-linuxbridge-agent.service neutron-dhcp-agent.service \
     neutron-metadata-agent.service
   ```

     对于网络选项2，还启用并启动第3层服务：

   ```shell
    systemctl enable neutron-l3-agent.service
    systemctl start neutron-l3-agent.service
   ```

   

## 计算节点

### 安装的组件

```shell
 yum install openstack-neutron-linuxbridge ebtables ipset
```

### 配置公共部件

 网络公用组件配置包括身份验证机制，消息队列和插件。 

>  默认配置文件因分发而异。您可能需要添加这些部分和选项，而不是修改现有的部分和选项。另外，`...`配置摘要中的省略号（）表示您应保留的潜在默认配置选项。 

- 编辑`/etc/neutron/neutron.conf`文件并完成以下操作： 

  - 在本`[database]`节中，注释掉所有`connection`选项，因为计算节点不会直接访问数据库。

  - 在该`[DEFAULT]`部分中，配置`RabbitMQ` 消息队列访问：

    ```shell
    [DEFAULT]
    # ...
    transport_url = rabbit://openstack:RABBIT_PASS@controller
    auth_strategy = keystone
    ```

  - 在`[keystone_authtoken]`部分中，配置身份服务访问：

    ```shell
    [keystone_authtoken]
    # ...
    www_authenticate_uri = http://controller:5000
    auth_url = http://controller:5000
    memcached_servers = controller:11211
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    project_name = service
    username = neutron
    password = NEUTRON_PASS
    ```

     替换`NEUTRON_PASS`为您`neutron` 在身份服务中为用户选择的密码。 

  - 在该`[oslo_concurrency]`部分中，配置锁定路径：

    ```shell
    [oslo_concurrency]
    # ...
    lock_path = /var/lib/neutron/tmp
    ```

    

    

### 配置网络

### 配置Linux网桥代理

Linux网桥代理为实例构建第2层（桥接和交换）虚拟网络基础结构并处理安全组。

- 编辑`/etc/neutron/plugins/ml2/linuxbridge_agent.ini`文件并完成以下操作： 

  - 在本`[linux_bridge]`节中，将提供者虚拟网络映射到提供者物理网络接口： 

    ```shell
    [linux_bridge]
    physical_interface_mappings = provider:PROVIDER_INTERFACE_NAME
    ```

     替换`PROVIDER_INTERFACE_NAME`为基础提供商物理网络接口的名称,能上网的接口名称。 例如：ens33

  - 在本`[vxlan]`节中，启用VXLAN覆盖网络，配置处理覆盖网络的物理网络接口的IP地址，并启用第2层填充：

    ```shell
    [vxlan]
    enable_vxlan = true
    local_ip = OVERLAY_INTERFACE_IP_ADDRESS
    l2_population = true
    ```

     替换OVERLAY_INTERFACE_IP_ADDRESS为计算节点的管理IP地址。 

  - 在该`[securitygroup]`部分中，启用安全组并配置Linux网桥iptables防火墙驱动程序： 

    ```shell
    [securitygroup]
    # ...
    enable_security_group = true
    firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
    ```

  - 通过验证以下所有`sysctl`值是否设置为确保Linux操作系统内核支持网桥过滤器1 

  - vi /etc/sysctl.conf  增加以下两行后输入命令 sysctl -p

    ```shell
    net.bridge.bridge-nf-call-iptables=1
    net.bridge.bridge-nf-call-ip6tables=1
    ```

    

### 配置计算服务使用网络服务

- 编辑`/etc/nova/nova.conf`文件并完成以下操作： 

  - 在该`[neutron]`部分中，配置访问参数： 

    ```shell
    [neutron]
    # ...
    url = http://controller:9696
    auth_url = http://controller:5000
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    region_name = RegionOne
    project_name = service
    username = neutron
    password = NEUTRON_PASS
    ```

     替换`NEUTRON_PASS`为您`neutron` 在身份服务中为用户选择的密码。 

### 最终确定安装

1. 重新启动计算服务： 

   ```shell
    systemctl restart openstack-nova-compute.service
   ```

2. 启动Linux网桥代理，并将其配置为在系统引导时启动：

   ```shell
    systemctl enable neutron-linuxbridge-agent.service
    systemctl start neutron-linuxbridge-agent.service
   ```

## 验证

在控制节点下验证

```
$ openstack network agent list
```

```
+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+
| ID                                   | Agent Type         | Host       | Availability Zone | Alive | State | Binary                    |
+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+
| f49a4b81-afd6-4b3d-b923-66c8f0517099 | Metadata agent     | controller | None            | True  | UP    | neutron-metadata-agent    |
| 27eee952-a748-467b-bf71-941e89846a92 | Linux bridge agent   | controller   | None          | True  | UP    | neutron-linuxbridge-agent |
| 08905043-5010-4b87-bba5-aedb1956e27a | Linux bridge agent   | compute1     | None          | True  | UP    | neutron-linuxbridge-agent |
| 830344ff-dc36-4956-84f4-067af667a0dc | L3 agent          | controller | nova            | True  | UP    | neutron-l3-agent       |
| dd3644c9-1a3a-435a-9282-eb306b4b0391 | DHCP agent        | controller | nova             | True  | UP    | neutron-dhcp-agent      |
+--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+
```

# Dashboard服务配置

## 安装和配置的部件

> 注意：
>
> 注意默认配置文件因分发而异。您可能需要添加这些部分和选项，而不是修改现有的部分和选项。另外，`...`配置摘要中的省略号（）表示您应保留的潜在默认配置选项。

- 1.安装软件包： 

  ```shell
  $ yum install openstack-dashboard
  ```

- 2.编辑 `/etc/openstack-dashboard/local_settings` 文件并完成以下操作：

  - 配置仪表板以在`controller`节点上使用OpenStack服务 ：

    ```shell
    OPENSTACK_HOST = "controller"
    ```

  - 允许主机访问仪表板：

    ```shell
    ALLOWED_HOSTS = ['*', 'two.example.com']
    ```

    > 注意
    >
    > ALLOWED_HOSTS也可以是['*']以接受所有主机。这对于开发工作可能有用，但是可能不安全，因此不应在生产中使用。有关 更多信息，请参见 https://docs.djangoproject.com/en/dev/ref/settings/#allowed-hosts。

  - 配置`memcached`会话存储服务：

    ```shell
    SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
    
    CACHES = {
    	'default': {
         'BACKEND':'django.core.cache.backends.memcached.MemcachedCache',
             'LOCATION': 'controller:11211',
        }
    }
    ```

    > 注意：
    >
    > 注释掉任何其他会话储存配置。

  - 启用身份API版本3：

    ```shell
    $ OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
    ```

  - 启用对域的支持：

    ```shell
    $ OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
    ```

  - 配置API版本：

    ```shell
    OPENSTACK_API_VERSIONS = {
        "identity": 3,
        "image": 2,
        "volume": 3,
    }
    ```

  - 配置`Default`为通过仪表板创建的用户的默认域：

    ```shell
    OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"
    ```

  - 配置`user`为通过仪表板创建的用户的默认角色：

    ```shell
    OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
    ```

  - 如果选择网络选项1，请禁用对第3层网络服务的支持：

    ```shell
    OPENSTACK_NEUTRON_NETWORK = {
        ...
        'enable_router': False,
        'enable_quotas': False,
        'enable_distributed_router': False,
        'enable_ha_router': False,
        'enable_lb': False,
        'enable_firewall': False,
        'enable_vpn': False,
        'enable_fip_topology_check': False,
    }
    ```

  - （可选）配置时区：

    ```shell
    TIME_ZONE = "TIME_ZONE"
    ```

    替换`TIME_ZONE`为适当的时区标识符。有关更多信息，请参见[时区列表](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)。

- 3.`/etc/httpd/conf.d/openstack-dashboard.conf`如果不包含以下内容，请添加以下行 。

  ```shell
  WSGIApplicationGroup %{GLOBAL}
  ```

## 最终确定安装

- 重新启动Web服务器和会话存储服务：

  ```shell
  $ systemctl restart httpd.service memcached.service
  ```

  > 注意:
  >
  > 如果当前未运行，该命令将启动每个服务。`systemctl restart`

# Cinder服务配置

## 控制节点

#### 1.创建数据库

- 使用数据库访问客户端以`root`用户身份连接到数据库服务器： 

  ```shell
  $ mysql -u root -p
  ```

- 创建`cinder`数据库： 

  ```shell
  MariaDB [(none)] CREATE DATABASE cinder;
  ```

- 授予对`neutron`数据库的适当访问权限，`NEUTRON_DBPASS`并用合适的密码代替 ： 

  ```shell
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
    IDENTIFIED BY 'CINDER_DBPASS';
  MariaDB [(none)]> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
    IDENTIFIED BY 'CINDER_DBPASS';
  ```

- 退出数据库访问客户端。 

#### 2. SOURCE`admin`凭据以获得对仅限管理的CLI命令的访问权限：

```shell
$ source admin-openrc
```

#### 3. 要创建服务凭证，请完成以下步骤 

- 创建`cinder`用户： 

  ```shell
  $ openstack user create --domain default --password-prompt cinder
  
  User Password:
  Repeat User Password:
  +---------------------+----------------------------------------------+
  | Field          | Value                            |
  +---------------------+----------------------------------------------+
  | domain_id        | default                          |
  | enabled         | True                            |
  | id		      | 9d7e33de3e1a498390353819bc7d245d         |
  | name           | cinder              	             |
  | options         | {}                             |
  | password_expires_at | None                            |
  +---------------------+----------------------------------------------+
  ```

- `admin`向`cinder`用户添加角色：

  ```shell
  $ openstack role add --project service --user cinder admin
  ```

  >  **注意**:该命令不提供任何输出。

- 创建`cinder`服务实体：

  ```shell
  $ openstack service create --name cinderv2 \
    --description "OpenStack Block Storage" volumev2
    
  +-------------+-------------------------------------------+
  | Field     | Value                         |
  +-------------+-------------------------------------------+
  | description | OpenStack Block Storage             |
  | enabled    | True                          |
  | id       | eb9fd245bdbc414695952e93f29fe3ac       |
  | name      | cinderv2                       |
  | type      | volumev2                       |
  +-------------+-------------------------------------------+
  ```

  ```shell
  $ openstack service create --name cinderv3 \
    --description "OpenStack Block Storage" volumev3
    
  +-------------+-------------------------------------------+
  | Field     | Value                         |
  +-------------+-------------------------------------------+
  | description | OpenStack Block Storage             |
  | enabled    | True                          |
  | id       | ab3bbbef780845a1a283490d281e7fda       |
  | name      | cinderv3                       |
  | type      | volumev3                       |
  +-------------+-------------------------------------------+
  ```

  > **注意**:块储存服务需要两个服务实体。

#### 4.创建块储存服务API终端：

```shell
$ openstack endpoint create --region RegionOne \
  volumev2 public http://controller:8776/v2/%\(project_id\)s

+---------------+----------------------------------------------------+
|Field       | Value                               |
+---------------+----------------------------------------------------+
| enabled     | True                                |
| id		  | 513e73819e14460fb904163f41ef3759             |
| interface    | public       	 	 		       	     |
| region      | RegionOne          	 	               |
| region_id    | RegionOne   		                    |
| service_id   | f71529314dab4a4d8eca427e701d209e             |
| service_name  | cinderv2                             |
| service_type  | volumev2                             |
| url        | http://controller:8776/v2/%(project_id)s        |
+--------------+-----------------------------------------------------+

$ openstack endpoint create --region RegionOne \
  volumev2 internal http://controller:8776/v2/%\(project_id\)s

+--------------+----------------------------------------------------+
| Field      | Value                               |
+--------------+----------------------------------------------------+
| enabled	  | True                                |
| id        | 09753b537ac74422a68d2d791cf3714f             |
| interface   | internal                  		 	    |
| region     | RegionOne                        	  |
| region_id   | RegionOne                   		   |
| service_id   | f71529314dab4a4d8eca427e701d209e    		   |
| service_name | cinderv2                             |
| service_type | volumev2                             |
| url       | http://controller:8776/v2/%(project_id)s        |
+--------------+----------------------------------------------------+

$ openstack endpoint create --region RegionOne \
  volumev2 admin http://controller:8776/v2/%\(project_id\)s

+---------------+----------------------------------------------------+
|Field       | Value                               |
+---------------+----------------------------------------------------+
| enabled     | True                                |
| id		  | e652cf84dd334f359ae9b045a2c91d96             |
| interface    | admin                               |
| region      | RegionOne          	 	               |
| region_id    | RegionOne                  			 |
| service_id   | eb9fd245bdbc414695952e93f29fe3ac             |
| service_name  | cinderv2                             |
| service_type  | volumev2                             |
| url        | http://controller:8776/v2/%(project_id)s        |
+--------------+-----------------------------------------------------+
```

#### 

```shell
$ openstack endpoint create --region RegionOne \
  volumev3 public http://controller:8776/v3/%\(project_id\)s

+---------------+----------------------------------------------------+
|Field       | Value                               |
+---------------+----------------------------------------------------+
| enabled     | True                                |
| id		  | 03fa2c90153546c295bf30ca86b1344b             |
| interface    | public       	 	 		       	     |
| region      | RegionOne          	 	               |
| region_id    | RegionOne   		                    |
| service_id   | ab3bbbef780845a1a283490d281e7fda             |
| service_name  | cinderv3                             |
| service_type  | volumev3                             |
| url        | http://controller:8776/v3/%(project_id)s        |
+--------------+-----------------------------------------------------+

$ openstack endpoint create --region RegionOne \
  volumev3 internal http://controller:8776/v3/%\(project_id\)s

+--------------+----------------------------------------------------+
| Field      | Value                               |
+--------------+----------------------------------------------------+
| enabled	  | True                                |
| id        | 94f684395d1b41068c70e4ecb11364b2             |
| interface   | internal                  		 	    |
| region     | RegionOne                        	  |
| region_id   | RegionOne                   		   |
| service_id   | ab3bbbef780845a1a283490d281e7fda    		   |
| service_name | cinderv3                             |
| service_type | volumev3                             |
| url       | http://controller:8776/v3/%(project_id)s        |
+--------------+----------------------------------------------------+

$ openstack endpoint create --region RegionOne \
  volumev3 admin http://controller:8776/v3/%\(project_id\)s

+---------------+----------------------------------------------------+
|Field       | Value                               |
+---------------+----------------------------------------------------+
| enabled     | True                                |
| id		  | 4511c28a0f9840c78bacb25f10f62c98             |
| interface    | admin                               |
| region      | RegionOne          	 	               |
| region_id    | RegionOne                  			 |
| service_id   | ab3bbbef780845a1a283490d281e7fda             |
| service_name  | cinderv3                             |
| service_type  | volumev3                             |
| url        | http://controller:8776/v3/%(project_id)s        |
+--------------+-----------------------------------------------------+
```

> **注意**:块存储服务需要每个服务实体的终端。

### 安装和配置组件：

- 1.安装：

  ```shell
  $ yum install openstack-cinder
  ```

- 

- 2.编辑文件`/etc/cinder/cinder.conf `并完成下列命令：

  - 在该`[database]`部分中，配置数据库访问： 

    ```shell
    [database]
    # ...
    connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder
    ```

     替换`CINDER_DBPASS`为您为数据库选择的密码。

  - 在该`[DEFAULT]`部分中，配置RabbitMQ消息队列： 

    ```shell
    [DEFAULT]
    # ...
    transport_url = rabbit://openstack:RABBIT_PASS@controller
    ```

     替换`RABBIT_PASS`为你在RabbitMQ中为openstack账户提供的密码。

  - 在该`[DEFAULT]`部分和`[keystone_authtoken]`部分中，配置身份服务访问：

    ```shell
    [DEFAULT]
    # ...
    auth_strategy = keystone
    
    [keystone_authtoken]
    # ...
    www_authenticate_uri = http://controller:5000
    auth_url = http://controller:5000
    memcached_servers = controller:11211
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    project_name = service
    username = cinder
    password = CINDER_PASS
    ```

     替换`CINDER_PASS`为您`cinder` 在身份服务中为用户选择的密码。 

    > **注意**:注释掉或删除[keystone_authtoken]部分中的任何其他选项。 

  - 在`[DEFAULT]`部分中配置my_ip选项来使用控制节点的网络管理接口：

    ```shell
    [DEFAULT]
    # ...
    my_ip = 10.0.0.11
    ```


- 3.在`[oslo_concurrency]`部分中，配置路径：

  ```shell
  [oslo_concurrency]
  # ...
  lock_path = /var/lib/cinder/tmp
  ```

- 4.同步块储存数据库：

  ```shell
  $ su -s /bin/sh -c "cinder-manage db sync" cinder
  ```

  > **注意**:忽视任何输出结果。 

### 配置计算服务来使用块储存

- 1.编辑`/etc/nova/nova.conf`文件并添加如下：

  ```shell
  [cinder]
  os_region_name = RegionOne
  ```

### 最终确定安装

- 1.重启计算API服务：

  ```shell
  $ systemctl restart openstack-nova-api.service
  ```

- 2.开启块储存服务并设置开机自启：

  ```shell
  $ systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
  $ systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service
  ```









## 安装并配置一个储存节点（在compute上完成）

### 准备

在你安装块储存服务前你必须先准备一块额外的储存设备(在虚拟机上添加一块硬盘)

>  在存储节点上完成下列步骤

- 1.安装支持实用程序包： 

  - 安装LVM包：

    ```shell
    $ yum install lvm2 device-mapper-persistent-data
    ```

  - 开启LVM服务并设置开机自启：

    ```shell
    $ systemctl enable lvm2-lvmetad.service
    $ systemctl start lvm2-lvmetad.service
    ```

    >  一些分布默认包括LVM服务


- 2.创建LVM物理卷`/dev/sdb`： 

  ```shell
  $ pvcreate /dev/sdb
  成功后显示：
  Physical volume "/dev/sdb" successfully created
  ```

- 3.创建LVM卷组`cinder-volumes`：

  ```shell
  $ vgcreate cinder-volumes /dev/sdb
  成功后显示：
  Volume group "cinder-volumes" successfully created
  ```

  块存储服务在此卷组中创建逻辑卷。

- 4.只有实例才能访问块存储卷。但是，底层操作系统管理与卷关联的设备。默认情况下，LVM卷扫描工具扫描`/dev`目录以查找包含卷的块存储设备。如果项目在其卷上使用LVM，扫描工具将检测这些卷并尝试缓存它们，这可能会导致底层操作系统和项目卷出现各种问题。必须重新配置LVM，以便仅扫描包含cinder卷卷组的设备。编辑`/etc/lvm/lvm.conf`文件并完成以下操作：

  - 在devices选项中，增加一个接受`/dev/sdb`设备并拒绝所有其他设备的筛选器：

    ```shell
    devices {
    ...
    filter = [ "a/sda/", "a/sdb/", "r/.*/"]
    ```

    筛选器数组中的每个项都以`for accept`或`r for reject`开头，并包含设备名称的正则表达式。数组必须以r/*/结尾才能拒绝任何剩余的设备。你可以使用`vgs-vvvv`命令来测试过滤器。

    ```shell
    $ systemctl enable lvm2-lvmetad.service
    $ systemctl start lvm2-lvmetad.service
    ```

    >  一些分布默认包括LVM服务

### 安装和配置组件

- 1.安装支持实用程序包： 

  ```shell
  $ yum install openstack-cinder targetcli python-keystone
  ```


- 2.编辑`/etc/cinder/cinder.conf`文件并完成下列操作： 

  - 在`[database]`选项，配置数据库：

    ```shell
    [database]
    # ...
    connection = mysql+pymysql://cinder:CINDER_DBPASS@controller/cinder
    ```

    替换`CINDER_DBPASS`为你的用于块储存数据库的密码。

  - 在`[DEFAULT]`选项中配置`RabbitMQ`消息队列：

    ```shell
    [DEFAULT]
    # ...
    transport_url = rabbit://openstack:RABBIT_PASS@controller
    ```

    替换`RABBIT_PASS`为你的用于RABBITMQ中openstack账户的密码。

  - 在`[DEFAULT]`和`[keystone_authtoken]`选项中配置身份认证：

    ```shell
    [DEFAULT]
    # ...
    auth_strategy = keystone
    
    [keystone_authtoken]
    # ...
    www_authenticate_uri = http://controller:5000
    auth_url = http://controller:5000
    memcached_servers = controller:11211
    auth_type = password
    project_domain_name = default
    user_domain_name = default
    project_name = service
    username = cinder
    password = CINDER_PASS
    ```

    替换`CINDER_DBPASS`为你的用于块储存数据库的密码。

    >  去除在`[keystone_authtoken]`项目中所有除上面输入内容之外的其他内容

  - 在`[DEFAULT]`选项中配置`[my_ip]`选项：

    ```shell
    [DEFAULT]
    # ...
    my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
    ```

    替换`MANAGEMENT_INTERFACE_IP_ADDRESS`为你存储节点上的管理接口ip地址。

  - 在`[lvm]`选项中配置使用LVM驱动程序、`cinder-volume`卷组、iSCSI协议和适当的iSCSI服务配置LVM后端。如果`[lvm]`部分不存在，则创建它： 

    ```shell
    [lvm]
    volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
    volume_group = cinder-volumes
    target_protocol = iscsi
    target_helper = lioadm
    ```

  - 在`[DEFAULT]`选项中启用LVM后端：

    ```shell
    [DEFAULT]
    # ...
    enabled_backends = lvm
    ```

    > 注意 
    >
    > 后端名称是任意的。例如，本指南使用驱动程序的名称作为后端的名称。

  - 在`[DEFAULT]`选项中配置映像服务API的位置： 

    ```shell
    [DEFAULT]
    # ...
    glance_api_servers = http://controller:9292
    ```

  - 在`[oslo_concurrency]`选项中配置锁定路径：

    ```shell
    [oslo_concurrency]
    # ...
    lock_path = /var/lib/cinder/tmp
    ```

  ### 最终安装

- 4.启动块存储卷服务（包括其依赖项），并将其配置为在系统启动时启动： 

  ```shell
  $ systemctl enable openstack-cinder-volume.service target.service
  $ systemctl start openstack-cinder-volume.service target.service
  ```








## 安装并配置一个备份节点

（可选）安装和配置备份服务。为了简单起见，此配置使用块存储节点和对象存储（swift）驱动程序，因此依赖于对象存储服务。 

> 注意：在安装和配置备份服务之前，必须安装和配置存储节点。

### 安装并配置组件

> 注意：在块存储节点上执行以下步骤。

- 1.安装程序包： 

  ```shell
  $ yum install openstack-cinder
  ```


- 2.编辑`/etc/cinder/cinder.conf`文件并完成下列操作： 

  - 在`[DEFAULT]`选项中配置备份选项 ：

    ```shell
    [DEFAULT]
    # ...
    backup_driver = cinder.backup.drivers.swift
    backup_swift_url = SWIFT_URL
    ```

    > 将SWIFT_URL替换为对象存储服务的URL。URL可以通过显示对象存储API终结点来找到： 

    ```shell
    $ openstack catalog show object-store
    ```

### 最终安装

启动块存储备份服务，并将其配置为在系统启动时启动： 

```shell
$ systemctl enable openstack-cinder-backup.service
$ systemctl start openstack-cinder-backup.service
```







## 验证cinder操作 

验证块存储服务的操作。

> 注意：在控制器节点上执行这些命令。 

- 1.获取管理员凭据以访问仅限管理员的CLI命令：

  ```shell
  $ source admin-openrc
  ```


- 2.列出服务组件以验证每个进程是否成功启动：  

- ```
  $ openstack volume service list
  +------------------+------------+------+---------+-------+----------------------------+
  | Binary        | Host     | Zone | Status  | State | Updated_at            |
  +------------------+------------+------+---------+-------+----------------------------+
  | cinder-scheduler | controller | nova | enabled | up   | 2016-09-30T02:27:41.000000 |
  | cinder-volume   | block@lvm  | nova | enabled | up   | 2016-09-30T02:27:46.000000 |
  | cinder-backup   | controller | nova | enabled | up   | 2016-09-30T02:27:41.000000 |
  +------------------+------------+------+---------+-------+----------------------------+
  ```

  

## 

